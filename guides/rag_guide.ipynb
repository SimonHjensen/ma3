{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/rag_overview.png\" alt=\"LLM\" width=\"800\"/> <br>\n",
    "Image from HuggingFace's \"<i><a href=\"https://huggingface.co/learn/cookbook/rag_evaluation\">RAG Evaluation</a></i>\" by <a href=\"https://huggingface.co/m-ric\">Aymeric Roucher</a>.<br> Copyright Â© 2025. All rights reserved.\n",
    "</p>\n",
    "\n",
    "***\n",
    "Sources: <br>\n",
    "- [RAG Evaluation (HuggingFace)](https://huggingface.co/learn/cookbook/rag_evaluation)\n",
    "- [RAG: Retrieval-Augmented Generation (LangChain)](https://python.langchain.com/docs/concepts/rag/)\n",
    "- [RAG Retrieval](https://python.langchain.com/docs/concepts/retrieval/)\n",
    "- [Embedding Models (LangChain)](https://python.langchain.com/docs/concepts/embedding_models/)\n",
    "- [Building a Semantic Search Engine for Internal Documentation: A Comprehensive Guide](https://blog.devgenius.io/building-a-semantic-search-engine-for-internal-documentation-a-comprehensive-guide-270efa9a30a0)\n",
    "- [Build a Retrieval Augmented Generation (RAG) App: Part 1](https://python.langchain.com/docs/tutorials/rag/)\n",
    "\n",
    "\n",
    "## Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases. RAG is a technique used to address [a key limitation of models](https://arxiv.org/abs/2401.11817): Generative language models rely on fixed training datasets, which can lead to outdated or incomplete information - and they will likely not know the answer to domain-specific questions like confidential company information. When an LLM is presented with a query that it doesn't know the answer to, the best case scenario is that it will respond with \"I don't know.\" However, as generative models lack self-reflection and are trained to, well, generate, in many cases, the model will generate a response that is incorrect, irrelevant or directly misleading, a phenomenon known as *hallucinations*.\n",
    "\n",
    "### Understanding hallucinations\n",
    "Research (e.g., \"*[Hallucination is inevitable: An innate limitation of large language models](https://arxiv.org/abs/2401.11817)\"* (2024) by Xu, Z., Jain, S., & Kankanhalli, M.) shows that hallucinatory behavior is a fundamental limitation of LLMs. As autoregressive models, they generate text sequentially with no mechanism to verify factual accuracy against reality. Their knowledge is fixed in parameters during training and lacks external grounding during generation. Mathematical proofs show that even theoretically perfect LLMs cannot learn all computable functions, making some degree of hallucination inevitable. This limitation applies not just in theoretical frameworks but extends to all real-world LLM applications as well. One dire example is [how a lawyer used ChatGPT in court and cited fake cases, leading to a judge considering sanctions.](https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/?sh=3fd97317c7f3&ref=alhena.ai). When forced to find an answer outside its training data, the model simply invents one. \n",
    "\n",
    "| Query                                | LLM Response                                             | Error                                                                 |\n",
    "|--------------------------------------|----------------------------------------------------------|-----------------------------------------------------------------------|\n",
    "| What is Ann Frank's favorite smartphone? | Ann Frank's favorite smartphone is the iPhone 12 Pro Max. | Anne Frank died in 1945, and the iPhone 12 Pro Max was released in the 2020s. |\n",
    "\n",
    "\n",
    "Another interesting perspective is given by [Andrej Karpathy](https://karpathy.ai/), Eureka Labs and former Director of AI at Tesla, who [tweeted](https://x.com/karpathy/status/1733299213503787018?lang=es):\n",
    "\n",
    "> *\"I always struggle a bit with I'm asked about the \"hallucination problem\" in LLMs. Because, in some sense, hallucination is all LLMs do. They are dream machines. We direct their dreams with prompts. The prompts start the dream, and based on the LLM's hazy recollection of its training documents, most of the time the result goes someplace useful. It's only when the dreams go into deemed factually incorrect territory that we label it a \"hallucination\". It looks like a bug, but it's just the LLM doing what it always does.*\n",
    "\n",
    "> *At the other end of the extreme consider a search engine. It takes the prompt and just returns one of the most similar \"training documents\" it has in its database, verbatim. You could say that this search engine has a \"creativity problem\" - it will never respond with something new. An LLM is 100% dreaming and has the hallucination problem. A search engine is 0% dreaming and has the creativity problem. \\[...\\]\"*\n",
    "\n",
    "### Mitigating hallucinations\n",
    "While various approaches to mitigate hallucinatory behavior continue to develop, the underlying constraints ensure that some degree of hallucination will persist in any LLM implementation. When faced with a hallucinating model, we would like to feed it with more information. The intuitive way to accomplish this is to fine-tune the model on a specific dataset. However, this approach is costly and time-consuming, and it may not be feasible for all applications. And what if we have information that changes frequently, like news, product prices, or stock market data? Fine-tuning is not a practical solution in these cases because the model would need to be retrained every time the data changes. Broadly speaking, there are three ways to mitigate hallucinations:\n",
    "| Approach            | Pros                                                                 | Cons                                                                 |\n",
    "|---------------------|----------------------------------------------------------------------|---------------------------------------------------------------------|\n",
    "| **Fine-tuning**     | Reduces hallucinations by training on specific datasets, Can improve model performance in specific domains | Costly and time-consuming, Not feasible for frequently changing information |\n",
    "| **Self-reflection** | Models can admit when they don't know the answer, Can reduce incorrect or misleading responses | Still in early stages of development, Might block hallucinations but doesn't expand the models knowledge. |\n",
    "| **External grounding** | Provides models with external knowledge sources to ground responses, Can significantly reduce hallucinations | Requires access to up-to-date and relevant knowledge bases, May involve additional complexity in system design and maintenance |\n",
    "\n",
    "### RAG as external grounding\n",
    "The inherent limitation of hallucinary behaviour highlights why complementary approaches like RAG have become popular. With RAG, we can induce the model with external knowledge *at inference time* instead of training time, a very cost effective alternative to fine-tuning. By grounding LLM responses in external, verifiable knowledge sources, RAG provides a practical technique to reduce hallucinations and (hopefully) improve the reliability of AI-generated content.\n",
    "\n",
    "### How does RAG work?\n",
    "When given a query, before the LLM sees the input, RAG systems first search a knowledge base for information relevant to the query (using semantic similarity search via a [retriever](/docs/concepts/retrievers/)). The system then incorporates this retrieved information into the model's prompt. This means that the model can use the provided context to generate a response to the query. What we are really doing is *grounding* the model's response in the retrieved context, which helps to reduce hallucinations and improve the quality of the response.\n",
    "\n",
    "More specifically, if we follow the image above, the RAG system works as follows:\n",
    "\n",
    "| Phase               | Stage            | Steps                                                                 |\n",
    "|---------------------|------------------|----------------------------------------------------------------------|\n",
    "| **Pre-Production**  | Knowledge Base Preparation | 1. Start with a collection of documents/information sources <br> 2. Apply chunking methods to break documents into manageable pieces <br> 3. Embed documents using an embedding model <br> 4. Store embeddings in a vector database |\n",
    "| **Production**      | Retriever Stage  | 1. Receive user query <br> 2. (Optionally reformulate the user query to improve retrieval) <br> 3. Embed the user query using the same embedding model <br> 4. Find closest documents to the embedded query in the vector database <br> (5. Possibility to use metadata in search process) |\n",
    "| **Production**   | Reader Stage     | 1. Retrieve top k similar documents (Documents contain text content, vector embeddings, and metadata) <br> 2. Post-process and aggregate document contents into a context <br> (Can apply prompt compression techniques and/or reranking if needed) <br> 3. Build a prompt based on the user query and context (Consider prompt choice strategies) <br> 4. Pass the prompt to the LLM <br> 5. LLM generates a response from the query, system prompt and context <br> 6. (Optionally verify the generated response) <br> 7. Return the LLM answer to the user |\n",
    "\n",
    "***\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Key concepts\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/rag_concepts.png\" alt=\"LLM\" width=\"800\"/> <br>\n",
    "Image from <a href=\"https://www.langchain.com/\">LangChain</a>'s \"<i><a href=https://python.langchain.com/docs/concepts/rag/\">Retrieval augmented generation (RAG)</a></i>\".<br> Copyright Â© 2025. All rights reserved.\n",
    "</p>\n",
    "\n",
    "0. **Creating a knowledge base**: Collect and embed relevant documents.\n",
    "\n",
    "1. **Retrieval system**: Retrieve relevant information from a knowledge base.\n",
    "\n",
    "2. **Using external knowledge**: Pass retrieved information to a model.\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "### Creating a knowledge base\n",
    "\n",
    "The first step in setting up a RAG system is to create a knowledge base. This involves collecting and *embedding* relevant documents that the model can use to ground its responses. Consider the table below as inspiration for the types of data sources you might use in different domains:\n",
    "\n",
    "| Use Case                | Relevant Data Sources                                                                 |\n",
    "|-------------------------|----------------------------------------------------------------------------------------|\n",
    "| **Customer Support**    | FAQ documents, product manuals, support tickets, knowledge base articles               |\n",
    "| **Healthcare**          | Medical journals, clinical trial data, patient records, drug information databases      |\n",
    "| **Finance**             | Financial reports, stock market data, economic indicators, investment research reports |\n",
    "| **E-commerce**          | Product catalogs, customer reviews, sales data, inventory records                       |\n",
    "| **Education**           | Textbooks, lecture notes, research papers, online course materials                      |\n",
    "| **Legal**               | Case law databases, legal statutes, contracts, legal opinions                           |\n",
    "| **News**                | News articles, press releases, social media feeds, news agency reports                  |\n",
    "| **Travel**              | Travel guides, hotel reviews, flight schedules, destination information                 |\n",
    "\n",
    "<br>\n",
    "\n",
    "> The quality of the knowledge base is crucial for the performance of the RAG system. The documents should be relevant, up-to-date, and diverse enough to cover a wide range of topics. This alone presents a whole set of challenges, such as data collection, cleaning, updating, and deduplication, which are beyond the scope of this guide.\n",
    "\n",
    "> The knowledge base can contain not only text but also other types of content, such as images, audio, or video. In this case, the retrieval system should be able to handle multimodal data and provide the model with the relevant information. This, however, adds another layer of complexity to the system, and lies beyond the scope of this guide.\n",
    "\n",
    "### Embedding documents\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/semantic_space.png\" alt=\"LLM\" width=\"800\"/> <br>\n",
    "Image from <a href=\"https://www.langchain.com/\">LangChain</a>'s \"<i><a href=https://python.langchain.com/docs/concepts/embedding_models/\">Embedding models</a></i>\".<br> Copyright Â© 2025. All rights reserved.\n",
    "</p>\n",
    "\n",
    "Imagine being able to capture the essence of any text - a tweet, document, or book - in a single, compact representation. This is the power of embedding models, which lie at the heart of many retrieval systems. Embedding models transform human language into a format that machines can understand and compare with speed and accuracy. These models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text's semantic meaning. Embeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding.\n",
    "\n",
    "Each embedding is essentially a set of coordinates, often in a high-dimensional space. In this space, the position of each point (embedding) reflects the meaning of its corresponding text. Just as similar words might be close to each other in a thesaurus, similar concepts end up close to each other in this embedding space. This allows for intuitive comparisons between different pieces of text. By reducing text to these numerical representations, we can use simple mathematical operations to quickly measure how alike two pieces of text are, regardless of their original length or structure. Some common similarity metrics include:\n",
    "\n",
    "- Cosine Similarity: Measures the cosine of the angle between two vectors.\n",
    "- Euclidean Distance: Measures the straight-line distance between two points.\n",
    "- Dot Product: Measures the projection of one vector onto another.\n",
    "\n",
    "**Remember how we used BERT embeddings in MA2?** BERT is just one example of an embedding (or an *encoder*) model. Embedding models are trained on large amounts of text data to learn the relationships between words, sentences, and documents. In contrast to generative models, which produce text, embedding models are discriminative models that map text to a fixed-size vector.\n",
    "\n",
    "**In a RAG settings, the retrieval system uses these embeddings to find the most relevant documents to a given query.**\n",
    "\n",
    "#### Steps to create a knowledge base\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/indexing.png\" alt=\"LLM\" width=\"800\"/> <br>\n",
    "Image from <a href=\"https://www.langchain.com/\">LangChain</a>'s \"<i><a href=https://python.langchain.com/docs/tutorials/rag/\">Build a Retrieval Augmented Generation (RAG) App: Part 1</a></i>\".<br> Copyright Â© 2025. All rights reserved.\n",
    "</p>\n",
    "\n",
    "Essentially, we need to accomplish four things to create a knowledge base, as shown in the image above.\n",
    "\n",
    "1. **Load**: First we need to load our data. This is done with Document Loaders.\n",
    "2. **Split**: Text splitters break large documents into smaller chunks. This is useful (and often needed) both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "3. **Embed**: We need to convert our text into embeddings. This is done using an `embedding model`.\n",
    "4. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a `VectorStore`.\n",
    "\n",
    "#### Chunking strategies\n",
    "What is the best way to split documents into chunks? There are several strategies, each with its own trade-offs:\n",
    "\n",
    "| Strategy                | Description                                                                                                                                                                                                 | Pros                                                                 | Cons                                                                 |\n",
    "|-------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------|---------------------------------------------------------------------|\n",
    "| Fixed-size chunking     | Text splitting with a specific chunk size and optional chunk overlap. This approach is the most common and straightforward.                                                                                  | Simple to implement, Consistent chunk sizes                          | May split important context, Not context-aware                      |\n",
    "| Recursive chunking      | Iterating default separators until one of them produces the preferred chunk size. Default separators include [\"\\n\\n\", \"\\n\", \" \", \"\"]. This chunking method uses hierarchical separators to keep paragraphs, followed by sentences and then words, together as much as possible. | Maintains logical structure, Flexible                                | Can be computationally expensive, May still split important context |\n",
    "| Semantic chunking       | Splitting text in a way that groups sentences based on the semantic similarity of their embeddings. Embeddings of high semantic similarity are closer together than those of low semantic similarity. This results in context-aware chunks. | Context-aware, Preserves semantic meaning                            | Requires embedding computation, More complex to implement           |\n",
    "| Document-based chunking | Splitting based on document structure. This splitter can utilize Markdown text, images, tables and even Python code classes and functions as ways of determining structure. In doing so, large documents can be chunked and processed by the LLM. | Preserves document structure, Versatile                              | May require custom logic for different document types               |\n",
    "| Agentic chunking        | Leverages agentic AI by allowing the LLM to determine appropriate document splitting based on semantic meaning as well as content structure such as paragraph types, section headings, step-by-step instructions and more. This chunker is experimental and attempts to simulate human reasoning when processing long documents. | Highly context-aware, Simulates human reasoning                      | Experimental, May be inconsistent, Computationally expensive         |\n",
    "\n",
    "The choice of the chunking strategy is an often overlooked but extremely important part of building a RAG system. The right strategy can significantly improve the quality of the generated responses, while the wrong strategy can lead to irrelevant or incorrect information being passed to the model. The best strategy depends on the nature of the documents in the knowledge base and the requirements of the application. Generally speaking, you will want to start off with something simple like fixed-size chunking and then experiment with more advanced strategies to see if they improve the performance of the system.\n",
    "\n",
    "\n",
    "### Retrieval system\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/semantic_search.png\" alt=\"LLM\" width=\"800\"/> <br>\n",
    "Image from <a href=\"https://blog.devgenius.io\">Dev Genius</a>'s <i><a href=https://blog.devgenius.io/building-a-semantic-search-engine-for-internal-documentation-a-comprehensive-guide-270efa9a30a0\">Building a Semantic Search Engine for Internal Documentation</a></i>\" by <a href=\"https://www.linkedin.com/in/timothyurista/\">Tim Urista</a><br> Copyright Â© 2024. All rights reserved.\n",
    "</p>\n",
    "\n",
    "Remember that RAG is an abbreviation for \"*Retrieval*-Augmented Generation.\" A retrieval system is a key component of RAG that searches for relevant information in a knowledge base. In the prior section we learned how we can use embeddings to represent documents in a high-dimensional space. The retrieval system uses these embeddings to find the most relevant documents to a given query. The system then passes this information to the model, which uses it to generate a response.\n",
    "\n",
    "In practie, we\n",
    "\n",
    "1. **Receive a query**: The retrieval system receives a query from the user.\n",
    "2. **Embed the query**: The system converts the query into an embedding using the same embedding model used to encode the documents.\n",
    "3. **Search for relevant documents**: The system compares the query embedding to the embeddings of the chunked documents in the knowledge base to find the most similar document chunks.\n",
    "4. **Retrieve the top k document chunks**: The system retrieves the top k most similar documents and passes them to the model.\n",
    "\n",
    "Retrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets. These systems accommodate various data formats:\n",
    "\n",
    "- Unstructured text (e.g., documents) is often stored in vector stores or lexical search indexes.\n",
    "- Structured data is typically housed in relational or graph databases with defined schemas.\n",
    "\n",
    "Despite the growing diversity in data formats, modern AI applications increasingly aim to make all types of data accessible through natural language interfaces. Models play a crucial role in this process by translating natural language queries into formats compatible with the underlying search index or database. This translation enables more intuitive and flexible interactions with complex data structures. There is of course much more to this topic, but this is a high-level overview of how retrieval systems work in RAG. We encourage you to check out [this guide of retrieval systems](https://python.langchain.com/docs/concepts/retrieval/) for a more in-depth look at the topic.\n",
    "\n",
    "#### Using external knowledge\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/simple_rag.webp\" alt=\"LLM\" width=\"800\"/> <br>\n",
    "Image from <a href=\"https://www.clarifai.com/\">ClarifAI</a>'s <i><a href=https://www.clarifai.com/blog/what-is-rag-retrieval-augmented-generation\">Building a Semantic Search Engine for Internal Documentation</a></i>\" by <a href=\"https://kelk.ai/\">Ian Kelk</a><br> Copyright Â© 2025. All rights reserved.\n",
    "</p>\n",
    "\n",
    "With our document embeddings and our a retrieval system in place, we need to pass knowledge from this system to the model. \n",
    "A RAG pipeline typically achieves this following these steps:\n",
    "1. Receive an input query.\n",
    "2. Use the retrieval system to search for relevant information based on the query.\n",
    "3. Incorporate the retrieved information into the prompt sent to the LLM.\n",
    "4. Generate a response that leverages the retrieved context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Any\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from decouple import config\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from langgraph.graph import START, StateGraph\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "import litellm\n",
    "from litellm import completion\n",
    "import instructor\n",
    "from instructor import Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve secrets\n",
    "\n",
    "**NOTE**: To load secrets from a `.env` file like this, you need to make sure that you run python from the same directory as the `.env` file. If you are running this notebook in a different directory, you will need to adjust `Jupyter: Notebook File Root` in your VSC settings to the directory where the `.env` file is located, i.e. `${workspaceFolder}`. See this [StackOverflow post](https://stackoverflow.com/a/59595999) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WX_API_KEY = config(\"WX_API_KEY\")\n",
    "WX_PROJECT_ID = config(\"WX_PROJECT_ID\")\n",
    "WX_API_URL = \"https://us-south.ml.cloud.ibm.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authenticate and initialize LLM\n",
    "\n",
    "As in Mandatory Assignment 2, we need to authenticate with our API KEY and PROJECT ID\n",
    "\n",
    "In contrast to MA2, we are now using `LangChain` to connect to WatsonX, instead of using IBMÂ´s own WatsonXAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = WatsonxLLM(\n",
    "\n",
    "        model_id= \"ibm/granite-3-8b-instruct\",\n",
    "        url=WX_API_URL,\n",
    "        apikey=WX_API_KEY,\n",
    "        project_id=WX_PROJECT_ID,\n",
    "\n",
    "        params={\n",
    "            GenParams.DECODING_METHOD: \"greedy\",\n",
    "            GenParams.TEMPERATURE: 0,\n",
    "            GenParams.MIN_NEW_TOKENS: 5,\n",
    "            GenParams.MAX_NEW_TOKENS: 1_000,\n",
    "            GenParams.REPETITION_PENALTY:1.2\n",
    "        }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate text using the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.outputs.llm_result.LLMResult'>\n",
      "generations=[[Generation(text=\"\\nI'm an artificial intelligence and don't have feelings, but I'm here to help you. How can I assist you today?\", generation_info={'finish_reason': 'eos_token'})]] llm_output={'token_usage': {'generated_token_count': 31, 'input_token_count': 5}, 'model_id': 'ibm/granite-3-8b-instruct', 'deployment_id': None} run=[RunInfo(run_id=UUID('e1b86329-a76d-4624-8056-f010c2e45f72'))] type='LLMResult'\n"
     ]
    }
   ],
   "source": [
    "llm_result = llm.generate([\"Hi how are you?\"])\n",
    "\n",
    "print(type(llm_result))\n",
    "print(llm_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want the text and not the metadata, you can use the `invoke` method. This method takes the same arguments as `generate` but returns only the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\n",
      "I'm an artificial intelligence and don't have feelings, but I'm here to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "llm_result = llm.invoke(\"Hi how are you?\")\n",
    "\n",
    "print(type(llm_result))\n",
    "print(llm_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or stream text using the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "My favorite breed of dog is the Golden Retriever, a large-sized gun dog originating from Scotland in the 19th century. They were initially bred for retrieving game such as waterfowl during hunting expeditions due to their exceptional intelligence, strength, and gentle temperament. Over time, they have become popular family pets because of these same qualities that make them excellent working dogs. Here are some reasons I adore this magnificent breed:\n",
      "\n",
      "1) Affectionate Nature: Golden Retrievers are known for being incredibly affectionate and loyal companions. They thrive on human interaction and form strong bonds with their families, often displaying unwavering devotion towards children and other animals alike. Their friendly demeanor makes them great choices for households where love and warmth are abundant.\n",
      "\n",
      "2) Intelligence & Trainability: Ranked among the most intelligent dog breeds by various canine experts, Goldens excel at learning new commands and tricks quickly. This trait not only enhances obedience training but also opens up opportunities for engaging activities like agility courses or therapy work. Moreover, their eagerness to please ensures smooth cooperation throughout any task.\n",
      "\n",
      "3) Versatility: Beyond companionship, Golden Retrievers possess remarkable versatility. Besides serving as dedicated service animals (guide dogs), search-and-rescue partners, and assistance dogs for people with disabilities, they also participate enthusiastically in numerous sports including conformation shows, field trials, tracking tests, and dock diving competitions. \n",
      "\n",
      "4) Healthy Appetite & Exercise Needs: With moderate exercise requirements and healthy appetites, Goldens fit well into active lifestyles while still maintaining balance within sedentary routines too. Regular walks, playtime sessions, and mental stimulation keep them happy and physically sound - contributing significantly to overall longevity.\n",
      "\n",
      "5) Gorgeous Coat: Lastly, let's talk about appearance! Golden Retrievers boast beautiful medium-length coats ranging from light gold to dark cream shades. These rich colors complement their muscular build perfectly, giving off an air of elegance and grace even when simply relaxing around the home. Plus, regular brushing helps distribute natural oils through their fur promoting skin health and reducing shedding.\n",
      "\n",
      "In summary, my admiration for Golden Retrievers stems from their amiable nature, impressive intellect, adaptability across different roles, balanced energy levels, and striking physical attributes. Each aspect contributes uniquely to what many consider one of the world's finest all-around pet options."
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\n",
    "    \"Describe your favorite breed of dog and why it is your favorite.\"\n",
    "):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load documents\n",
    "\n",
    "We can use `langchain-community`'s `TextLoader` to load text documents from our local system. This is a convenient way to quickly build a knowledge base. However, we could also use `WebBaseLoader` to load directly from online sources. You can see all the available types of loaders [in the LangChain documentation here](https://python.langchain.com/docs/integrations/document_loaders/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/madeup_company.md'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = TextLoader(\"data/madeup_company.md\").load()[0]\n",
    "document.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split documents\n",
    "\n",
    "Since we are dealing with a markdown file, we can use `MarkdownHeaderTextSplitter`. This splitter will split the document into chunks based on the headers in the markdown file. This is a good way to maintain the structure of the document and ensure that the chunks are coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\"), (\"####\", \"Header 4\")]\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "chunks = text_splitter.split_text(document.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'About MadeUpCompany'}, page_content='MadeUpCompany is a pioneering technology firm founded in 2010, specializing in cloud computing, data analytics, and machine learning. Headquartered in San Francisco, California, we have a global presence with satellite offices in New York, London, and Tokyo. Our mission is to empower businesses and individuals with cutting-edge technology that enhances efficiency, scalability, and innovation.  \\nWith a diverse team of experts from various industriesâincluding AI research, cybersecurity, and enterprise software developmentâwe push the boundaries of whatâs possible. Our commitment to continuous improvement, security, and customer success has earned us recognition as a leader in the tech space.'),\n",
       " Document(metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Our Values'}, page_content=\"At MadeUpCompany, we believe in:  \\n- Innovation â Continuously developing and refining solutions that meet the evolving needs of businesses.\\n- Security & Privacy â Implementing world-class security protocols to protect our customers' data.\\n- Customer-Centric Approach â Designing intuitive, powerful tools that make complex technology accessible.\\n- Sustainability â Ensuring our infrastructure is energy-efficient and environmentally responsible.\"),\n",
       " Document(metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Products and Services'}, page_content='We offer a comprehensive suite of cloud-based solutions that streamline operations, enhance decision-making, and power AI-driven insights. Our core products include CloudMate, DataWiz, and Custom AI Solutions.'),\n",
       " Document(metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Products and Services', 'Header 3': 'CloudMate â Secure and Scalable Cloud Storage'}, page_content='CloudMate is our flagship cloud storage solution, designed for businesses of all sizes. Features include:\\n- â Seamless data migration with automated backups\\n- â Military-grade encryption and multi-factor authentication\\n- â Role-based access control for enterprise security\\n- â AI-powered file organization and search capabilities'),\n",
       " Document(metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Products and Services', 'Header 3': 'DataWiz â Advanced Data Analytics'}, page_content='DataWiz transforms raw data into actionable insights using cutting-edge machine learning models. Features include:\\n- ð Predictive analytics for demand forecasting and customer behavior modeling\\n- ð Real-time dashboards with customizable reporting\\n- ð API integrations with popular business intelligence tools\\n- ð Automated anomaly detection for fraud prevention and operational efficiency'),\n",
       " Document(metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Products and Services', 'Header 3': 'Custom AI Solutions'}, page_content='We provide tailored machine learning models to optimize business workflows, automate repetitive tasks, and enhance decision-making. From NLP-based chatbots to AI-driven recommendation engines, we develop bespoke AI solutions for various industries.'),\n",
       " Document(metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Pricing'}, page_content='We offer flexible pricing plans to meet the needs of individuals, small businesses, and large enterprises.'),\n",
       " Document(metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Pricing', 'Header 3': 'CloudMate Plans'}, page_content='Our secure and scalable cloud storage service, CloudMate, is available in the following plans:\\n- Basic: $9.99/month â 100GB storage, essential security features\\n- Professional: $29.99/month â 1TB storage, enhanced security, priority support\\n- Enterprise: Custom pricing â Unlimited storage, advanced compliance tools, dedicated account manager'),\n",
       " Document(metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Pricing', 'Header 3': 'DataWiz Plans'}, page_content='Our advanced data analytics platform, DataWiz, offers the following plans:\\n- Starter: $49/month â Basic analytics, limited AI insights\\n- Growth: $99/month â Advanced machine learning models, predictive analytics\\n- Enterprise: Custom pricing â Full AI customization, dedicated data scientists\\n- Custom AI Solutions â Pricing is determined based on project scope and complexity. Contact our sales team for a personalized quote.'),\n",
       " Document(metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Technical Support'}, page_content='Our award-winning customer support team is available 24/7 to assist with any technical issues. Support channels include:\\n- ð Toll-free phone support\\n- ð¬ Live chat assistance\\n- ð§ Email support with guaranteed response within 6 hours\\n- ð Comprehensive FAQ and user guides available on our website\\n- ð¥ Community forum for peer-to-peer discussions and best practices  \\nMost technical issues are resolved within 24 hours, ensuring minimal downtime for your business.'),\n",
       " Document(metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Security and Compliance'}, page_content='Security is at the heart of everything we do. MadeUpCompany adheres to the highest security and regulatory standards, including:  \\n- ð GDPR, HIPAA, and SOC 2 Compliance â Ensuring global security and data protection compliance.\\n- ð End-to-End Encryption â Protecting data in transit and at rest with AES-256 encryption.\\n- ð Zero Trust Architecture â Implementing rigorous access control and continuous authentication.\\n- ð DDoS Protection & Advanced Threat Detection â Safeguarding against cyber threats with AI-powered monitoring.  \\nOur team continuously updates security measures to stay ahead of evolving cyber risks.'),\n",
       " Document(metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Account Management'}, page_content='Managing your MadeUpCompany services is simple and intuitive via our online portal. Customers can:  \\n- âï¸ Upgrade or downgrade plans at any time\\n- âï¸ Access billing history and download invoices\\n- âï¸ Manage multiple users and set role-based permissions\\n- âï¸ Track storage and analytics usage in real time  \\nFor enterprise accounts, we offer dedicated account managers who provide strategic guidance and personalized support.'),\n",
       " Document(metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Refund and Cancellation Policy'}, page_content=\"We stand by the quality of our services and offer a 30-day money-back guarantee on all plans.  \\nIf you're not satisfied, you can request a full refund within the first 30 days.\\nAfter 30 days, you may cancel your subscription at any time, and weâll issue a prorated refund based on your remaining subscription period.\\nEnterprise contracts include a flexible exit clause, ensuring fair terms for long-term clients.\\nUpcoming Features\"),\n",
       " Document(metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Roadmap'}, page_content=\"We are constantly evolving and introducing new features based on customer feedback. Hereâs whatâs coming soon:  \\n- ð AI-Driven Data Insights â DataWiz will introduce automated trend forecasting powered by deep learning.\\n- ð Collaboration Tools for CloudMate â Enhanced real-time document editing and team workspaces for seamless collaboration.\\n- ð Zero-Knowledge Encryption â An optional feature for businesses requiring absolute data confidentiality.  \\nWe value our customers' input and prioritize updates that deliver the most impact.\"),\n",
       " Document(metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Why Choose Us?'}, page_content=\"- âï¸ Over 1 million satisfied users worldwide\\n- âï¸ Trusted by Fortune 500 companies\\n- âï¸ Featured in TechCrunch, Forbes, and Wired as a top innovator\\n- âï¸ Unmatched customer support and security  \\nWhether you're a startup, an enterprise, or an individual user, MadeUpCompany provides the tools you need to thrive in the digital age.\"),\n",
       " Document(metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Contact'}, page_content='For more information, visit our website at www.madeupcompany.com or contact our sales team at sales@madeupcompany.com. ð')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess chunks\n",
    "\n",
    "If we looks at the metadata for each chunk, we see that it is neatly split into header types (Header 1, header 2, etc.) and the text content. This is useful! We can write a function to add the header type(s) to the text content itself, so that the model can use this information to generate better responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_documents_with_headers(chunks):\n",
    "    \"\"\"\n",
    "    Creates a new list of Document objects with page_content prepended with headers\n",
    "    in [Header1/Header2/Header3]: format\n",
    "    \n",
    "    Returns new objects rather than modifying the original chunks\n",
    "    \"\"\"\n",
    "    updated_chunks = []\n",
    "    \n",
    "    for doc in chunks:\n",
    "        # Create a deep copy of the document to avoid modifying the original\n",
    "        new_doc = deepcopy(doc)\n",
    "        \n",
    "        # Get all headers that exist in metadata\n",
    "        headers = []\n",
    "        for i in range(1, 4):\n",
    "            key = f'Header {i}'\n",
    "            if key in new_doc.metadata:\n",
    "                headers.append(new_doc.metadata[key])\n",
    "        \n",
    "        # Create the header prefix and update page_content\n",
    "        if headers:\n",
    "            prefix = f\"[{'/'.join(headers)}]: \"\n",
    "            new_doc.page_content = prefix + \"\\n\" + new_doc.page_content\n",
    "        \n",
    "        updated_chunks.append(new_doc)\n",
    "    \n",
    "    return updated_chunks\n",
    "\n",
    "\n",
    "docs = update_documents_with_headers(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[About MadeUpCompany]: \n",
      "MadeUpCompany is a pioneering technology firm founded in 2010, specializing in cloud computing, data analytics, and machine learning. Headquartered in San Francisco, California, we have a global presence with satellite offices in New York, London, and Tokyo. Our mission is to empower businesses and individuals with cutting-edge technology that enhances efficiency, scalability, and innovation.  \n",
      "With a diverse team of experts from various industriesâincluding AI research, cybersecurity, and enterprise software developmentâwe push the boundaries of whatâs possible. Our commitment to continuous improvement, security, and customer success has earned us recognition as a leader in the tech space.\n",
      "\n",
      "[About MadeUpCompany/Our Values]: \n",
      "At MadeUpCompany, we believe in:  \n",
      "- Innovation â Continuously developing and refining solutions that meet the evolving needs of businesses.\n",
      "- Security & Privacy â Implementing world-class security protocols to protect our customers' data.\n",
      "- Customer-Centric Approach â Designing intuitive, powerful tools that make complex technology accessible.\n",
      "- Sustainability â Ensuring our infrastructure is energy-efficient and environmentally responsible.\n",
      "\n",
      "[About MadeUpCompany/Products and Services]: \n",
      "We offer a comprehensive suite of cloud-based solutions that streamline operations, enhance decision-making, and power AI-driven insights. Our core products include CloudMate, DataWiz, and Custom AI Solutions.\n",
      "\n",
      "[About MadeUpCompany/Products and Services/CloudMate â Secure and Scalable Cloud Storage]: \n",
      "CloudMate is our flagship cloud storage solution, designed for businesses of all sizes. Features include:\n",
      "- â Seamless data migration with automated backups\n",
      "- â Military-grade encryption and multi-factor authentication\n",
      "- â Role-based access control for enterprise security\n",
      "- â AI-powered file organization and search capabilities\n",
      "\n",
      "[About MadeUpCompany/Products and Services/DataWiz â Advanced Data Analytics]: \n",
      "DataWiz transforms raw data into actionable insights using cutting-edge machine learning models. Features include:\n",
      "- ð Predictive analytics for demand forecasting and customer behavior modeling\n",
      "- ð Real-time dashboards with customizable reporting\n",
      "- ð API integrations with popular business intelligence tools\n",
      "- ð Automated anomaly detection for fraud prevention and operational efficiency\n",
      "\n",
      "[About MadeUpCompany/Products and Services/Custom AI Solutions]: \n",
      "We provide tailored machine learning models to optimize business workflows, automate repetitive tasks, and enhance decision-making. From NLP-based chatbots to AI-driven recommendation engines, we develop bespoke AI solutions for various industries.\n",
      "\n",
      "[About MadeUpCompany/Pricing]: \n",
      "We offer flexible pricing plans to meet the needs of individuals, small businesses, and large enterprises.\n",
      "\n",
      "[About MadeUpCompany/Pricing/CloudMate Plans]: \n",
      "Our secure and scalable cloud storage service, CloudMate, is available in the following plans:\n",
      "- Basic: $9.99/month â 100GB storage, essential security features\n",
      "- Professional: $29.99/month â 1TB storage, enhanced security, priority support\n",
      "- Enterprise: Custom pricing â Unlimited storage, advanced compliance tools, dedicated account manager\n",
      "\n",
      "[About MadeUpCompany/Pricing/DataWiz Plans]: \n",
      "Our advanced data analytics platform, DataWiz, offers the following plans:\n",
      "- Starter: $49/month â Basic analytics, limited AI insights\n",
      "- Growth: $99/month â Advanced machine learning models, predictive analytics\n",
      "- Enterprise: Custom pricing â Full AI customization, dedicated data scientists\n",
      "- Custom AI Solutions â Pricing is determined based on project scope and complexity. Contact our sales team for a personalized quote.\n",
      "\n",
      "[About MadeUpCompany/Technical Support]: \n",
      "Our award-winning customer support team is available 24/7 to assist with any technical issues. Support channels include:\n",
      "- ð Toll-free phone support\n",
      "- ð¬ Live chat assistance\n",
      "- ð§ Email support with guaranteed response within 6 hours\n",
      "- ð Comprehensive FAQ and user guides available on our website\n",
      "- ð¥ Community forum for peer-to-peer discussions and best practices  \n",
      "Most technical issues are resolved within 24 hours, ensuring minimal downtime for your business.\n",
      "\n",
      "[About MadeUpCompany/Security and Compliance]: \n",
      "Security is at the heart of everything we do. MadeUpCompany adheres to the highest security and regulatory standards, including:  \n",
      "- ð GDPR, HIPAA, and SOC 2 Compliance â Ensuring global security and data protection compliance.\n",
      "- ð End-to-End Encryption â Protecting data in transit and at rest with AES-256 encryption.\n",
      "- ð Zero Trust Architecture â Implementing rigorous access control and continuous authentication.\n",
      "- ð DDoS Protection & Advanced Threat Detection â Safeguarding against cyber threats with AI-powered monitoring.  \n",
      "Our team continuously updates security measures to stay ahead of evolving cyber risks.\n",
      "\n",
      "[About MadeUpCompany/Account Management]: \n",
      "Managing your MadeUpCompany services is simple and intuitive via our online portal. Customers can:  \n",
      "- âï¸ Upgrade or downgrade plans at any time\n",
      "- âï¸ Access billing history and download invoices\n",
      "- âï¸ Manage multiple users and set role-based permissions\n",
      "- âï¸ Track storage and analytics usage in real time  \n",
      "For enterprise accounts, we offer dedicated account managers who provide strategic guidance and personalized support.\n",
      "\n",
      "[About MadeUpCompany/Refund and Cancellation Policy]: \n",
      "We stand by the quality of our services and offer a 30-day money-back guarantee on all plans.  \n",
      "If you're not satisfied, you can request a full refund within the first 30 days.\n",
      "After 30 days, you may cancel your subscription at any time, and weâll issue a prorated refund based on your remaining subscription period.\n",
      "Enterprise contracts include a flexible exit clause, ensuring fair terms for long-term clients.\n",
      "Upcoming Features\n",
      "\n",
      "[About MadeUpCompany/Roadmap]: \n",
      "We are constantly evolving and introducing new features based on customer feedback. Hereâs whatâs coming soon:  \n",
      "- ð AI-Driven Data Insights â DataWiz will introduce automated trend forecasting powered by deep learning.\n",
      "- ð Collaboration Tools for CloudMate â Enhanced real-time document editing and team workspaces for seamless collaboration.\n",
      "- ð Zero-Knowledge Encryption â An optional feature for businesses requiring absolute data confidentiality.  \n",
      "We value our customers' input and prioritize updates that deliver the most impact.\n",
      "\n",
      "[About MadeUpCompany/Why Choose Us?]: \n",
      "- âï¸ Over 1 million satisfied users worldwide\n",
      "- âï¸ Trusted by Fortune 500 companies\n",
      "- âï¸ Featured in TechCrunch, Forbes, and Wired as a top innovator\n",
      "- âï¸ Unmatched customer support and security  \n",
      "Whether you're a startup, an enterprise, or an individual user, MadeUpCompany provides the tools you need to thrive in the digital age.\n",
      "\n",
      "[About MadeUpCompany/Contact]: \n",
      "For more information, visit our website at www.madeupcompany.com or contact our sales team at sales@madeupcompany.com. ð\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.page_content, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the embedding model\n",
    "\n",
    "Note: The [supported embeddings models in WatsonX.ai](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models-embed.html?context=wx) are\n",
    "\n",
    "- `granite-embedding-107m-multilingual`\n",
    "- `granite-embedding-278m-multilingual`\n",
    "- `slate-30m-english-rtrvr-v2`\n",
    "- `slate-30m-english-rtrvr`\n",
    "- `slate-125m-english-rtrvr-v2`\n",
    "- `slate-125m-english-rtrvr`\n",
    "- `all-minilm-l6-v2`\n",
    "- `all-minilm-l12-v2`\n",
    "- `multilingual-e5-large`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_params = {}\n",
    "\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/granite-embedding-278m-multilingual\",\n",
    "    url=WX_API_URL,\n",
    "    project_id=WX_PROJECT_ID,\n",
    "    apikey=WX_API_KEY,\n",
    "    params=embed_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_vector_db = Chroma.from_documents(\n",
    "    collection_name=\"my_collection\",\n",
    "    embedding=watsonx_embedding,\n",
    "    persist_directory=\"my_vector_db\", # This will save the vector database to disk! Delete it if you want to start fresh.\n",
    "    documents=docs,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve documents with semantic search\n",
    "\n",
    "In LangChain we use `VectorStoreRetriever` as a sort of wrapper around the vector index. This retriever is used to search for documents based on their embeddings. We could also load a retriever directly. You can see all the available types of retrievers [in the LangChain documentation here](https://python.langchain.com/docs/integrations/retrievers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the vectorstore as a retriever\n",
    "retriever = local_vector_db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "ID: 2839b535-6c05-4c3c-9116-1344a19575c1\n",
      "Content: [About MadeUpCompany/Refund and Cancellation Policy]: \n",
      "We stand by the quality of our services and offer a 30-day money-back guarantee on all plans.  \n",
      "If you're not satisfied, you can request a full refund within the first 30 days.\n",
      "After 30 days, you may cancel your subscription at any time, and weâll issue a prorated refund based on your remaining subscription period.\n",
      "Enterprise contracts include a flexible exit clause, ensuring fair terms for long-term clients.\n",
      "Upcoming Features\n",
      "\n",
      "################################################################################\n",
      "ID: fa706126-3776-4525-a5dc-0546ef4e729c\n",
      "Content: [About MadeUpCompany/Pricing/CloudMate Plans]: \n",
      "Our secure and scalable cloud storage service, CloudMate, is available in the following plans:\n",
      "- Basic: $9.99/month â 100GB storage, essential security features\n",
      "- Professional: $29.99/month â 1TB storage, enhanced security, priority support\n",
      "- Enterprise: Custom pricing â Unlimited storage, advanced compliance tools, dedicated account manager\n",
      "\n",
      "################################################################################\n",
      "ID: b9548048-fb97-4627-b644-449d8cee5994\n",
      "Content: [About MadeUpCompany/Technical Support]: \n",
      "Our award-winning customer support team is available 24/7 to assist with any technical issues. Support channels include:\n",
      "- ð Toll-free phone support\n",
      "- ð¬ Live chat assistance\n",
      "- ð§ Email support with guaranteed response within 6 hours\n",
      "- ð Comprehensive FAQ and user guides available on our website\n",
      "- ð¥ Community forum for peer-to-peer discussions and best practices  \n",
      "Most technical issues are resolved within 24 hours, ensuring minimal downtime for your busine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Retrieve the most similar text\n",
    "retrieved_documents = retriever.invoke(\"Do you have a 30-day money-back guarantee?\")\n",
    "\n",
    "for document in retrieved_documents:\n",
    "    print(f\"{'#' * 80}\\nID: {document.id}\")\n",
    "    first_n_of_content = document.page_content[:500].replace('\\n\\n', ' ')\n",
    "    print(f\"Content: {first_n_of_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a RAG prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context: \n",
    "{context} \n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we can fill this with our question (our query) and our context (The retrieved documents). This is a simple example, but you can create more complex prompts by combining the query and context in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\nQuestion:\\nWhat is CloudMate?\\n\\nContext: \\n[About MadeUpCompany/Refund and Cancellation Policy]: \\nWe stand by the quality of our services and offer a 30-day money-back guarantee on all plans.  \\nIf you're not satisfied, you can request a full refund within the first 30 days.\\nAfter 30 days, you may cancel your subscription at any time, and weâll issue a prorated refund based on your remaining subscription period.\\nEnterprise contracts include a flexible exit clause, ensuring fair terms for long-term clients.\\nUpcoming Features \\n\\nAnswer:\\n\")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke(\n",
    "    input={\n",
    "        \"question\": \"What is CloudMate?\",\n",
    "        \"context\": retrieved_documents[0].page_content,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining our RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is CloudMate?\"\n",
    "\n",
    "retrieved_docs = local_vector_db.similarity_search(question)\n",
    "docs_content = \"\\n\\n\".join(f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(retrieved_docs))\n",
    "formated_prompt = prompt.invoke({\"question\": question, \"context\": docs_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "\n",
      "Question:\n",
      "What is CloudMate?\n",
      "\n",
      "Context: \n",
      "Document 1:\n",
      "[About MadeUpCompany/Products and Services/CloudMate â Secure and Scalable Cloud Storage]: \n",
      "CloudMate is our flagship cloud storage solution, designed for businesses of all sizes. Features include:\n",
      "- â Seamless data migration with automated backups\n",
      "- â Military-grade encryption and multi-factor authentication\n",
      "- â Role-based access control for enterprise security\n",
      "- â AI-powered file organization and search capabilities\n",
      "\n",
      "Document 2:\n",
      "[About MadeUpCompany/Pricing/CloudMate Plans]: \n",
      "Our secure and scalable cloud storage service, CloudMate, is available in the following plans:\n",
      "- Basic: $9.99/month â 100GB storage, essential security features\n",
      "- Professional: $29.99/month â 1TB storage, enhanced security, pr\n"
     ]
    }
   ],
   "source": [
    "print(formated_prompt.to_string()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm.invoke(formated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CloudMate is MadeUpCompany's flagship cloud storage solution, offering seamless data migration, military-grade encryption, role-based access control, and AI-powered file organization among its key features. It comes in different plans catering to varying business needs, ranging from basic to enterprise levels.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the RAG pipeline\n",
    "\n",
    "So, we just ran the RAG pipeline, but LangChain offers a neat way to do this with their `LangGraph` API. We will be using LangGraph in part two of the assignment, so letÂ´s get familiar with it.\n",
    "\n",
    "LangGraph is an extension of LangChain that allows you to build stateful, multi-actor systems using a graph-based structure. It's particularly useful for creating complex AI workflows where multiple components need to interact in a structured way. The typical pattern in LangGraph is to define nodes (functions or agents), the edges between them (which node should execute next), and conditions that determine the flow through the graph. This provides much more flexibility than linear chains while maintaining clear structure and control over the flow of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    \"\"\" A langgraph state for the application \"\"\"\n",
    "    question: str\n",
    "    context: list[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    \"\"\" Our retrieval step. We use our local vector database to retrieve similar documents to the question \"\"\"\n",
    "    retrieved_docs = local_vector_db.similarity_search(state[\"question\"], k=3) # NOTE: You can change k to retrieve fewer or more documents\n",
    "    return {\"context\": retrieved_docs} \n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    \"\"\" Our generation step. We use the retrieved documents to generate an answer to the question \"\"\"\n",
    "\n",
    "    # Format the prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    formated_prompt = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "\n",
    "    # Generate the answer\n",
    "    response = llm.invoke(formated_prompt)\n",
    "    return {\"answer\": response}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\") # Start at the retrieve step\n",
    "graph = graph_builder.compile() # Compile the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/urllib3/connectionpool.py:467\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[39m\n\u001b[32m    463\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    464\u001b[39m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[32m    465\u001b[39m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[32m    466\u001b[39m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m             \u001b[43msix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:3\u001b[39m, in \u001b[36mraise_from\u001b[39m\u001b[34m(value, from_value)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/urllib3/connectionpool.py:462\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m     httplib_response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    464\u001b[39m     \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[32m    465\u001b[39m     \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[32m    466\u001b[39m     \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/http/client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/ssl.py:1314\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1313\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/ssl.py:1166\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mTimeoutError\u001b[39m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mReadTimeoutError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/urllib3/connectionpool.py:799\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[39m\n\u001b[32m    797\u001b[39m     e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, e)\n\u001b[32m--> \u001b[39m\u001b[32m799\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/urllib3/util/retry.py:550\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43msix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/urllib3/packages/six.py:770\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m    769\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/urllib3/connectionpool.py:715\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[39m\n\u001b[32m    714\u001b[39m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m httplib_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[32m    726\u001b[39m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[32m    728\u001b[39m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/urllib3/connectionpool.py:469\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/urllib3/connectionpool.py:358\u001b[39m, in \u001b[36mHTTPConnectionPool._raise_timeout\u001b[39m\u001b[34m(self, err, url, timeout_value)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[32m    359\u001b[39m         \u001b[38;5;28mself\u001b[39m, url, \u001b[33m\"\u001b[39m\u001b[33mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m % timeout_value\n\u001b[32m    360\u001b[39m     )\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3. In Python 2 we have\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# to specifically catch it and throw the timeout error\u001b[39;00m\n",
      "\u001b[31mReadTimeoutError\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m display(Image(\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/langchain_core/runnables/graph.py:631\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding)\u001b[39m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[32m    626\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    627\u001b[39m     curve_style=curve_style,\n\u001b[32m    628\u001b[39m     node_colors=node_colors,\n\u001b[32m    629\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    630\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:250\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding)\u001b[39m\n\u001b[32m    244\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    245\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    246\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    247\u001b[39m         )\n\u001b[32m    248\u001b[39m     )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_color\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    254\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:368\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type)\u001b[39m\n\u001b[32m    362\u001b[39m         background_color = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m!\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackground_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    364\u001b[39m image_url = (\n\u001b[32m    365\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://mermaid.ink/img/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmermaid_syntax_encoded\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    366\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m?type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m&bgColor=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackground_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    367\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m    370\u001b[39m     img_bytes = response.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/requests/adapters.py:713\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    711\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request=request)\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[32m    715\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request=request)\n",
      "\u001b[31mReadTimeout\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)"
     ]
    }
   ],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is CloudMate?',\n",
       " 'context': [Document(id='0a76d96c-7548-4c9e-8cbf-c4507c1763cc', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Products and Services', 'Header 3': 'CloudMate â Secure and Scalable Cloud Storage'}, page_content='[About MadeUpCompany/Products and Services/CloudMate â Secure and Scalable Cloud Storage]: \\nCloudMate is our flagship cloud storage solution, designed for businesses of all sizes. Features include:\\n- â Seamless data migration with automated backups\\n- â Military-grade encryption and multi-factor authentication\\n- â Role-based access control for enterprise security\\n- â AI-powered file organization and search capabilities'),\n",
       "  Document(id='fa706126-3776-4525-a5dc-0546ef4e729c', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Pricing', 'Header 3': 'CloudMate Plans'}, page_content='[About MadeUpCompany/Pricing/CloudMate Plans]: \\nOur secure and scalable cloud storage service, CloudMate, is available in the following plans:\\n- Basic: $9.99/month â 100GB storage, essential security features\\n- Professional: $29.99/month â 1TB storage, enhanced security, priority support\\n- Enterprise: Custom pricing â Unlimited storage, advanced compliance tools, dedicated account manager'),\n",
       "  Document(id='d6088927-ed13-47eb-b809-9b0a4798d7cf', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Products and Services'}, page_content='[About MadeUpCompany/Products and Services]: \\nWe offer a comprehensive suite of cloud-based solutions that streamline operations, enhance decision-making, and power AI-driven insights. Our core products include CloudMate, DataWiz, and Custom AI Solutions.')],\n",
       " 'answer': 'CloudMate is a flagship cloud storage solution by MadeUpCompany, offering seamless data migration, military-grade encryption, role-based access control, and AI-powered file organization across various plans tailored for businesses of different sizes.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What is CloudMate?\"})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG evaluation\n",
    "\n",
    "\n",
    "We will built our own LLM-as-a-judge evaluation system for our LangGraph RAG system.\n",
    "\n",
    "We will evaluate for the following:\n",
    "\n",
    "1. Retrieval Quality (Checks if retrieved documents contain the information needed for answering)\n",
    "\n",
    "2. Answer Correctness (Ask an LLM whether the generated answer is sufficiently similar to the expected answer)\n",
    "\n",
    "3. Document Relevance (Evaluates how relevant each retrieved document is to the query)\n",
    "\n",
    "4. Hallucination Score (Identifies statements in the answer not supported by retrieved documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Question-Answer pairs (Gold standard examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_queries = [\n",
    "    \"When was MadeUpCompany founded and where is it headquartered?\",\n",
    "    \"What security features does CloudMate offer for enterprise customers?\",\n",
    "    \"How much does the Professional plan for CloudMate cost and what storage capacity does it include?\",\n",
    "    \"What analytics capabilities does DataWiz provide for business intelligence?\",\n",
    "    \"What compliance standards does MadeUpCompany adhere to?\",\n",
    "    \"What channels are available for technical support at MadeUpCompany?\",\n",
    "    \"What is MadeUpCompany's refund policy for the first 30 days?\",\n",
    "    \"What upcoming collaboration features is MadeUpCompany planning for CloudMate?\",\n",
    "    \"Where are MadeUpCompany's satellite offices located?\",\n",
    "    \"What four core values does MadeUpCompany believe in?\",\n",
    "    \"What professional backgrounds do MadeUpCompany's experts come from?\",\n",
    "    \"What does the Starter plan for DataWiz include and how much does it cost?\",\n",
    "    \"How quickly does MadeUpCompany promise to resolve technical issues?\",\n",
    "    \"What account management features are available through MadeUpCompany's online portal?\",\n",
    "    \"How many users does MadeUpCompany have and what publications have featured them?\",\n",
    "    \"What types of custom AI solutions does MadeUpCompany develop?\",\n",
    "    \"What encryption standard does MadeUpCompany use and where is it applied?\",\n",
    "    \"What are the specifications of the Basic plan for CloudMate?\",\n",
    "    \"What is the guaranteed response time for email support at MadeUpCompany?\",\n",
    "    \"What capabilities does DataWiz offer for fraud prevention?\",\n",
    "    \"How does MadeUpCompany's cancellation policy work after the 30-day period?\",\n",
    "    \"What machine learning capabilities will DataWiz introduce in upcoming features?\",\n",
    "    \"What pricing plan options are available for MadeUpCompany's Enterprise customers?\",\n",
    "    \"What does MadeUpCompany's sustainability value focus on?\",\n",
    "    \"How does MadeUpCompany describe its mission?\"\n",
    "]\n",
    "\n",
    "expected_responses = [\n",
    "    \"MadeUpCompany was founded in 2010 and is headquartered in San Francisco, California.\",\n",
    "    \"CloudMate offers military-grade encryption, multi-factor authentication, and role-based access control for enterprise security.\",\n",
    "    \"The Professional plan for CloudMate costs $29.99/month and includes 1TB of storage, enhanced security, and priority support.\",\n",
    "    \"DataWiz provides predictive analytics for demand forecasting and customer behavior modeling, real-time dashboards with customizable reporting, API integrations with popular business intelligence tools, and automated anomaly detection.\",\n",
    "    \"MadeUpCompany adheres to GDPR, HIPAA, and SOC 2 compliance standards for global security and data protection compliance.\",\n",
    "    \"MadeUpCompany offers toll-free phone support, live chat assistance, email support, comprehensive FAQ and user guides on their website, and a community forum for peer-to-peer discussions.\",\n",
    "    \"MadeUpCompany offers a 30-day money-back guarantee on all plans, allowing customers to request a full refund if they're not satisfied within the first 30 days.\",\n",
    "    \"MadeUpCompany is planning to introduce enhanced real-time document editing and team workspaces for seamless collaboration in CloudMate.\",\n",
    "    \"MadeUpCompany has satellite offices in New York, London, and Tokyo.\",\n",
    "    \"MadeUpCompany believes in innovation, security & privacy, a customer-centric approach, and sustainability.\",\n",
    "    \"MadeUpCompany's experts come from various industries including AI research, cybersecurity, and enterprise software development.\",\n",
    "    \"The Starter plan for DataWiz costs $49/month and includes basic analytics and limited AI insights.\",\n",
    "    \"MadeUpCompany resolves most technical issues within 24 hours, ensuring minimal downtime for businesses.\",\n",
    "    \"Through MadeUpCompany's online portal, customers can upgrade or downgrade plans, access billing history and download invoices, manage multiple users and set role-based permissions, and track storage and analytics usage in real time.\",\n",
    "    \"MadeUpCompany has over 1 million satisfied users worldwide and has been featured in TechCrunch, Forbes, and Wired as a top innovator.\",\n",
    "    \"MadeUpCompany provides tailored machine learning models including NLP-based chatbots and AI-driven recommendation engines to optimize business workflows, automate repetitive tasks, and enhance decision-making.\",\n",
    "    \"MadeUpCompany uses AES-256 encryption to protect data both in transit and at rest.\",\n",
    "    \"The Basic plan for CloudMate costs $9.99/month and includes 100GB storage and essential security features.\",\n",
    "    \"MadeUpCompany guarantees an email support response within 6 hours.\",\n",
    "    \"DataWiz provides automated anomaly detection for fraud prevention and operational efficiency.\",\n",
    "    \"After 30 days, customers may cancel their subscription at any time, and MadeUpCompany will issue a prorated refund based on the remaining subscription period.\",\n",
    "    \"DataWiz will introduce automated trend forecasting powered by deep learning.\",\n",
    "    \"MadeUpCompany offers custom pricing for Enterprise plans with unlimited storage, advanced compliance tools, and a dedicated account manager for CloudMate, and full AI customization with dedicated data scientists for DataWiz.\",\n",
    "    \"MadeUpCompany's sustainability value focuses on ensuring their infrastructure is energy-efficient and environmentally responsible.\",\n",
    "    \"MadeUpCompany's mission is to empower businesses and individuals with cutting-edge technology that enhances efficiency, scalability, and innovation.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a vanilla RAG evaluation system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create an **LLM Judge**.  We will use [litellm](https://pypi.org/project/litellm/) and [instructor](https://pypi.org/project/instructor/), because we want to make sure we get an exact output from the LLM.\n",
    "\n",
    "`Ã¬nstructor` allow us to pass in a `response_model` object to the generation method. The response from the llm will be in the same format as the `response_model` object. This is useful for evaluation purposes.\n",
    "\n",
    "Note also that it makes sense to use a heavier model for evaluation. The ideal is that we iteratively improve our smaller model, but we need a better/bigger model to evaluate the smaller model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a litellm client\n",
    "litellm.drop_params = True  # watsonx.ai doesn't support `json_mode`\n",
    "client = instructor.from_litellm(completion, mode=Mode.JSON)\n",
    "\n",
    "# create a response model - LLM is forced to return an object of this type\n",
    "class JudgeResponse(BaseModel):\n",
    "    reasoning: str = Field(description=\"Short one-sentence reason for score\")\n",
    "    score: Literal[0, .5, 1] = Field(description=\"Final score\")\n",
    "\n",
    "# define a function to call the judge\n",
    "def call_judge(prompt : str) -> JudgeResponse:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"watsonx/meta-llama/llama-3-3-70b-instruct\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        project_id=WX_PROJECT_ID,\n",
    "        apikey=WX_API_KEY,\n",
    "        api_base=WX_API_URL,\n",
    "        response_model=JudgeResponse,\n",
    "        # decoding_method=\"greedy\",\n",
    "        # temperature=0,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's write a `RagEvaluator` class that will use our LLM judge to score the output from our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    \"\"\"\n",
    "    A streamlined evaluator for RAG systems focusing on three key dimensions:\n",
    "    1. Retrieval Quality\n",
    "    2. Answer Correctness\n",
    "    3. Hallucination Detection\n",
    "    \"\"\"\n",
    "    def __init__(self, llm_func):\n",
    "        \"\"\"Initialize with an LLM.\"\"\"\n",
    "        self.llm_func = llm_func\n",
    "            \n",
    "    def evaluate_retrieval_quality(self, response: dict[str, Any], expected_answer: str, verbose : bool = False) -> JudgeResponse:\n",
    "        \"\"\"\n",
    "        Ask LLM if retrieved documents contain information needed for the expected answer.\n",
    "        \"\"\"\n",
    "        # Combine all retrieved document contents with clear formatting\n",
    "        retrieved_text = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(response.get('context', []))])\n",
    "        \n",
    "        prompt = f\"\"\"You are given a set of documents and a fact. Can the fact be found in the documents? Judge by the information, not the exact wording of the fact.\n",
    "        \n",
    "        - Respond with 1 if the fact is present (also if the fact can be pieced together from multiple documents).\n",
    "        - Respond with 0 if the fact is not present in any of the documents.\n",
    "        - Responds with 0.5 ff only part of the fact is present.\n",
    "        \n",
    "        Retrieved Documents: \n",
    "        {retrieved_text}\n",
    "\n",
    "        Fact:\n",
    "        {expected_answer}\n",
    "        \n",
    "        Can the fact be found in the documents? Respond as a JudgeResponse object with: \n",
    "        - a short reason (max 20 words)\n",
    "        - a score of 1, 0.5, or 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.llm_func(prompt)\n",
    "        if verbose:\n",
    "            print(f\"[evaluation_retrieval_quality] LLM response: {result}\")\n",
    "        return result\n",
    "    \n",
    "    def evaluate_answer_correctness(self, response: dict[str, Any], expected_answer: str, verbose : bool = False) -> JudgeResponse:\n",
    "        \"\"\"\n",
    "        Ask LLM to rate how correct/similar the generated answer is to the expected answer.\n",
    "        \"\"\"\n",
    "        generated_answer = response.get('answer', '')\n",
    "        \n",
    "        prompt = f\"\"\"You are evaluating a RAG system. You are given a question, an expected answer, and a generated answer. Is the generated answer as correct - or close to as correct - as the expected answer? \n",
    "        \n",
    "        - Respond with 1 if the answer is yes (also if the answer is more detailed than expected)\n",
    "        - Respond with 0 if the answer is no. \n",
    "        - respond with 0.5 if the generated answer is partially correct\n",
    "\n",
    "        Question:\n",
    "        {response.get('question', '')}\n",
    "        \n",
    "        Expected answer:\n",
    "        {expected_answer}\n",
    "        \n",
    "        Generated answer:\n",
    "        {generated_answer}\n",
    "        \n",
    "        Is the generated answer correct enough? Consider content correctness rather than exact wording. \n",
    "        Respond as a JudgeResponse object with: \n",
    "        - a short reason (max 20 words)\n",
    "        - a score of 1, 0.5, or 0.\"\"\"\n",
    "        \n",
    "        result = self.llm_func(prompt)\n",
    "        if verbose:\n",
    "            print(f\"[evaluation_answer_correctness] LLM response: {result}\")\n",
    "        return result\n",
    "    \n",
    "    def evaluate_hallucination(self, response: dict[str, Any], verbose : bool = False) -> JudgeResponse:\n",
    "        \"\"\"\n",
    "        Ask LLM to evaluate if the answer contains hallucinations.\n",
    "        \"\"\"\n",
    "        generated_answer = response.get('answer', '')\n",
    "        retrieved_text = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(response.get('context', []))])\n",
    "        \n",
    "        prompt = f\"\"\"You are evaluating a RAG system. Your task is to determine if the generated answer contains hallucinations. Hallucinations are any information that is not directly supported by the retrieved documents. Does the generated answer contain hallucinations? \n",
    "        \n",
    "        - If the answer is no, respond with 0. \n",
    "        - If the answer is yes, respond with 1.\n",
    "        - If the answer is partially hallucinated, respond with 0.5. \n",
    "        - If the generated answer states that it does not know, respond with 0.\n",
    "\n",
    "        Question: \n",
    "        {response.get('question', '')}\n",
    "        \n",
    "        Retrieved context (this is all the information the AI had access to):\n",
    "        {retrieved_text}\n",
    "        \n",
    "        Generated answer:\n",
    "        {generated_answer}\n",
    "        \n",
    "        Does the generated answer contain hallucinations? Respond as a JudgeResponse object with: \n",
    "        - a short reason (max 20 words)\n",
    "        - a score of 1, 0.5, or 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.llm_func(prompt)\n",
    "        if verbose:\n",
    "            print(f\"[evaluation_hallucination] LLM response: {result}\")\n",
    "        return result\n",
    "    \n",
    "    def evaluate(self, response: dict[str, Any], expected_answer: str, verbose : bool = False) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate a RAG response across all three dimensions.\n",
    "        \"\"\"\n",
    "        # Get scores for each dimension\n",
    "        retrieval_score = self.evaluate_retrieval_quality(response, expected_answer, verbose=verbose)\n",
    "        correctness_score = self.evaluate_answer_correctness(response, expected_answer, verbose=verbose)\n",
    "        hallucination_score = self.evaluate_hallucination(response, verbose=verbose)\n",
    "        \n",
    "        return {\n",
    "            \"query\": response.get(\"question\", \"\"),\n",
    "            \"retrieved_context\": response.get(\"context\", []),\n",
    "            \"generated_answer\": response.get(\"answer\", \"\"),\n",
    "            \"expected_answer\": expected_answer,\n",
    "            \"retrieval_quality\": retrieval_score.score,\n",
    "            \"answer_correctness\": correctness_score.score, \n",
    "            \"hallucination_score\": hallucination_score.score,  # Lower is better\n",
    "\n",
    "            # keep the reasoning for manual inspection\n",
    "            \"retrieval_quality_reasoning\": retrieval_score.reasoning,\n",
    "            \"answer_correctness_reasoning\": correctness_score.reasoning,\n",
    "            \"hallucination_reasoning\": hallucination_score.reasoning\n",
    "        }\n",
    "\n",
    "\n",
    "def evaluate_rag_system(graph, test_queries, expected_responses, evaluator, verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluate a RAG system on a test set.\n",
    "    \n",
    "    Args:\n",
    "        graph: The LangGraph RAG system with invoke method\n",
    "        test_queries: List of questions to test\n",
    "        expected_responses: List of expected answers\n",
    "        evaluator: The RAG evaluator object\n",
    "        \n",
    "    Returns:\n",
    "        Evaluation results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for query, expected in tqdm(zip(test_queries, expected_responses), total=len(test_queries)):\n",
    "\n",
    "        # Get RAG response\n",
    "        response = graph.invoke({\"question\": query})\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_result = evaluator.evaluate(response, expected, verbose=verbose)\n",
    "        results.append(eval_result)\n",
    "    \n",
    "    # Calculate average scores\n",
    "    avg_metrics = {\n",
    "        \"retrieval_quality\": np.mean([r[\"retrieval_quality\"] for r in results]),\n",
    "        \"answer_correctness\": np.mean([r[\"answer_correctness\"] for r in results]),\n",
    "        \"hallucination\": np.mean([r[\"hallucination_score\"] for r in results])\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"individual_results\": results,\n",
    "        \"scores\": avg_metrics,\n",
    "        \"num_queries\": len(test_queries)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact present in Document 1' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='Identical content' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â         | 1/25 [00:10<04:02, 10.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Answer is directly supported' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact matches Document 1' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='More detailed than expected' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â         | 2/25 [00:20<03:53, 10.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Added details not in context' score=1\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact found in Document 1' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='Matches expected answer' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|ââ        | 3/25 [00:30<03:41, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Matches retrieved context' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact fully described' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='Generated answer is more detailed' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|ââ        | 4/25 [00:40<03:30, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Answer is fully supported' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact mentioned in Document 1' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='More detailed than expected' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|ââ        | 5/25 [00:52<03:34, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Answer is directly supported' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Exact match found' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='Similar content, extra detail' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|âââ       | 6/25 [01:04<03:31, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Answer matches retrieved context' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact stated in Document 1' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='Generated answer is more detailed' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|âââ       | 7/25 [01:18<03:37, 12.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Answer is mostly supported' score=0.5\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact mentioned in roadmap' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='More detailed than expected' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|ââââ      | 8/25 [01:28<03:15, 11.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='No unsupported info' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact stated in Document 1' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='Same content, minor wording difference' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|ââââ      | 9/25 [01:38<02:55, 10.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Answer is directly supported' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact matches Document 1' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='Identical content' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|ââââ      | 10/25 [01:47<02:39, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Answer matches retrieved context' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact present in Document 1' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='Essentially same content' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|âââââ     | 11/25 [01:57<02:25, 10.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Answer is directly supported' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact found in Document 1' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='More detailed than expected' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|âââââ     | 12/25 [02:08<02:17, 10.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='includes unsupported features' score=1\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact stated in Document 1' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='Missing downtime detail' score=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|ââââââ    | 13/25 [02:17<02:01, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Answer is directly supported' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact found in Document 1' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='More detailed than expected' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|ââââââ    | 14/25 [02:28<01:53, 10.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Answer is fully supported' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact matches Document 1' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='Generated answer is similarly correct' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|ââââââ    | 15/25 [02:39<01:43, 10.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Matches retrieved context' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact is fully present' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='Similar content, slightly different wording' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|âââââââ   | 16/25 [02:49<01:33, 10.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Answer is directly supported' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Explicitly mentioned' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='More detailed, equally correct' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|âââââââ   | 17/25 [02:59<01:21, 10.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Answer directly supported' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact fully present' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='More detailed than expected' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|ââââââââ  | 18/25 [03:09<01:11, 10.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Added unsupported security features' score=1\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Email response time stated' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='Matches expected content' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|ââââââââ  | 19/25 [03:19<01:00, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Directly supported by Document 1' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact mentioned in Document 1' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='More detailed than expected' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|ââââââââ  | 20/25 [03:30<00:51, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Mentions unsupported features' score=1\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact fully stated' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='Generated answer is more detailed' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|âââââââââ | 21/25 [03:40<00:41, 10.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Accurate, based on retrieved context' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact mentioned in roadmap' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='More detailed than expected' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|âââââââââ | 22/25 [03:53<00:33, 11.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Adding encryption to ML capabilities' score=1\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Partial info found' score=0.5\n",
      "[evaluation_answer_correctness] LLM response: reasoning='Partial correctness' score=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|ââââââââââ| 23/25 [04:05<00:22, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Mentions customizable plans, not in context' score=1\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact stated in Document 1' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='Exact match' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|ââââââââââ| 24/25 [04:19<00:12, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Answer is directly supported' score=0\n",
      "[evaluation_retrieval_quality] LLM response: reasoning='Fact present' score=1\n",
      "[evaluation_answer_correctness] LLM response: reasoning='Generated answer is more detailed' score=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 25/25 [04:29<00:00, 10.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation_hallucination] LLM response: reasoning='Partial match, added wording' score=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'retrieval_quality': 0.98, 'answer_correctness': 0.96, 'hallucination': 0.28}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results = evaluate_rag_system(\n",
    "    graph, \n",
    "    sample_queries,\n",
    "    expected_responses,\n",
    "    evaluator=RAGEvaluator(llm_func=call_judge),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "results[\"scores\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQoJJREFUeJzt3XlUVfX+//HXiVkQHEAcQsQcwkj7ilZSBg7hlJl1b6iVE97CIVOsruZ1zCIrTe06lfNNEys1ZyVNHDBTkwYlc8ZueM1ZUxHh8/vDxfl1BBQVPbh7PtY6a3k++7P3fm/cZ/Pis4djM8YYAQAAWMRdzi4AAACgKBFuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBugFtoxowZstls9perq6sqVKigdu3aaffu3QXON27cONlsNoWFhV11+fv371fv3r0VGhoqb29veXp6qkqVKnr++ef19ddf61oPID9w4IBDfVe+hg4deiObXShDhw6VzWa7ZcuXpHPnzmno0KFau3Ztnmm5/zcHDhy4pTXkJysrS5MnT1b9+vVVpkwZlShRQsHBwWrTpo0WLFhw2+sBrMbV2QUAfwXTp0/XvffeqwsXLmjjxo1666239PXXX+vnn39W6dKl8/SfNm2aJGnHjh3avHmzHnrooTx9Fi1apA4dOsjf319xcXGqW7euPDw8tGfPHn3++edq3LixvvrqKzVp0uSa9b388svq0KFDnva77777Bra2+Dh37pyGDRsmSYqKinKY1qpVK23atEkVKlS47XW98MILmj9/vvr06aNhw4bJw8ND+/bt04oVK7Ry5Uq1bdv2ttcEWAnhBrgNwsLCVK9ePUmXf8lmZ2dryJAhWrhwobp06eLQd+vWrfr+++/VqlUrLV26VFOnTs0Tbvbu3av27dvrvvvu01dffSVfX1/7tMjISMXGxmrt2rX5Bqf8VK5cWQ8//PBNbuWdJSAgQAEBAbd9vfv371diYqIGDx5sD16S1KRJE/3jH/9QTk7ObavFGKMLFy7Iy8vrtq0TuB04LQU4QW7Q+d///pdn2tSpUyVJ77zzjiIiIjR37lydO3fOoc/o0aN17tw5TZgwwSHY/FlUVJTq1KlTJPX26dNH3t7eOn36dJ5pMTExCgwMVFZWliQpMTFR0dHRqlChgry8vBQaGqr+/fvrjz/+uOZ6CjoVVqVKFXXu3Nn+/vfff1ePHj1Uq1Yt+fj4qFy5cmrcuLHWr19v73PgwAF7eBk2bJj9VFvucgo6LTVt2jTVqVNHnp6eKlOmjNq2bau0tDSHPp07d5aPj4/27Nmjli1bysfHR0FBQerXr58yMzOvuo3Hjh2TpAJHjO66y/GwfPLkSfXr109Vq1aVh4eHypUrp5YtW+rnn3+29zl+/Lh69OihSpUqyd3dXVWrVtXAgQPz1GKz2dSrVy9NmjRJoaGh8vDw0MyZMyVJu3fvVocOHVSuXDl5eHgoNDRU48ePd5g/JydHI0aMUM2aNeXl5aVSpUqpdu3aGjt27FW3GbjdCDeAE+zfv1+SVKNGDYf28+fP69NPP1X9+vUVFhamrl276syZM/rss88c+iUlJalChQr2kHSzcnJydOnSpTyvXF27dtW5c+c0b948h/lOnjypL7/8Us8//7zc3NwkXf4l2bJlS02dOlUrVqxQnz59NG/ePLVu3bpIapUu/zKXpCFDhmjp0qWaPn26qlatqqioKPv1NRUqVNCKFSskSbGxsdq0aZM2bdqkQYMGFbjchIQExcbG6r777tP8+fM1duxY/fDDD2rQoEGea6SysrL05JNPqkmTJvryyy/VtWtXffDBBxo5cuRVaw8NDVWpUqU0bNgwffTRR1e95ufMmTN69NFHNXnyZHXp0kWLFy/WpEmTVKNGDWVkZEiSLly4oEaNGmnWrFmKj4/X0qVL9fzzz+vdd9/V008/nWeZCxcu1MSJEzV48GCtXLlSDRs21M6dO1W/fn399NNPGjVqlJYsWaJWrVqpd+/eDqNL7777roYOHar27dtr6dKlSkxMVGxsrE6ePHnVbQZuOwPglpk+fbqRZL755huTlZVlzpw5Y1asWGHKly9vHnvsMZOVleXQf9asWUaSmTRpkjHGmDNnzhgfHx/TsGFDh36enp7m4YcfzrO+7Oxsk5WVZX9lZ2dftb79+/cbSQW+1q9fb+9bt25dExER4TD/hAkTjCTz448/5rv8nJwck5WVZZKTk40k8/3339unDRkyxFx5CJJkhgwZkmc5wcHBplOnTgVux6VLl0xWVpZp0qSJadu2rb39999/L3CZuf83+/fvN8YYc+LECePl5WVatmzp0C89Pd14eHiYDh062Ns6depkJJl58+Y59G3ZsqWpWbNmgXXmWrp0qfH397f/nMuWLWv+/ve/m0WLFjn0Gz58uJFkkpKSClzWpEmT8q1l5MiRRpJZtWqVvU2S8fPzM8ePH3fo26xZM3P33XebU6dOObT36tXLeHp62vs/8cQT5oEHHrjm9gHOxsgNcBs8/PDDcnNzU8mSJdW8eXOVLl1aX375pVxdHS97mzp1qry8vNSuXTtJko+Pj/7+979r/fr1V727KtfTTz8tNzc3+6t3796Fqu+VV17Rli1b8rweeOABe58uXbooJSVFu3btsrdNnz7dPsqUa9++ferQoYPKly8vFxcXubm5KTIyUpLynN65GZMmTVLdunXl6ekpV1dXubm5afXq1Te8jk2bNun8+fMOp78kKSgoSI0bN9bq1asd2m02W57RqNq1a+vgwYPXXFfLli2Vnp6uBQsW6NVXX9V9992nhQsX6sknn1SvXr3s/ZYvX64aNWqoadOmBS5rzZo18vb21t/+9jeH9tztuLLuxo0bO1yLdeHCBa1evVpt27ZViRIlHEbuWrZsqQsXLuibb76RJD344IP6/vvv1aNHD61cuTLf05RAcUC4AW6DWbNmacuWLVqzZo1eeuklpaWlqX379g599uzZo3Xr1qlVq1YyxujkyZM6efKk/ZdW7h1U0uULgPP7JTpq1Ch7MLked999t+rVq5fn5ePjY+/z3HPPycPDQzNmzJAk7dy5U1u2bHG4IPrs2bNq2LChNm/erBEjRmjt2rXasmWL5s+fL+nyabeiMHr0aHXv3l0PPfSQvvjiC33zzTfasmWLmjdvfsPruNq1MBUrVrRPz1WiRAl5eno6tHl4eOjChQuFWp+Xl5eeeuopvffee0pOTtaePXtUq1YtjR8/Xjt27JB0+dqia92xduzYMZUvXz7PbfXlypWTq6trnrqv3L5jx47p0qVL+vDDDx2CsZubm1q2bClJOnr0qCRpwIABev/99/XNN9+oRYsWKlu2rJo0aaKtW7cWapuB24W7pYDbIDQ01H59TKNGjZSdna0pU6bo888/dwgvxhh9/vnn+vzzz/MsY+bMmRoxYoRcXFz0+OOPa/z48dq6davDdTf33HPPLduG0qVLq02bNpo1a5ZGjBih6dOny9PT0yGkrVmzRr/99pvWrl1rH62RVOhrMjw8PPK9IPfKX9CffPKJoqKiNHHiRIf2M2fOXMcWOSpbtqwk2a9l+bPffvtN/v7+N7zswqhcubJefPFF9enTRzt27NB9992ngIAA/frrr1edr2zZstq8ebOMMQ4B58iRI7p06VKeuq8MQaVLl5aLi4teeOEF9ezZM991hISESJJcXV0VHx+v+Ph4nTx5Ul999ZXeeOMNNWvWTIcOHVKJEiVuZNOBIsfIDeAE7777rkqXLq3BgwcrJydH2dnZmjlzpu655x59/fXXeV79+vVTRkaGli9fLknq27evSpQooZ49e97UL/Tr1aVLF/32229atmyZPvnkE7Vt21alSpWyT8/9xenh4eEw3+TJkwu1/CpVquiHH35waFuzZo3Onj3r0Gaz2fKs44cfftCmTZsc2nL7FGY0p0GDBvLy8tInn3zi0P7rr79qzZo1hXpeUGGcOXMmz/bkyj2lVrFiRUlSixYt9Msvv2jNmjUFLq9JkyY6e/asFi5c6NA+a9Ys+/SrKVGihBo1aqTt27erdu3a+Y7g5Qa/PytVqpT+9re/qWfPnjp+/LhTHoYIFISRG8AJSpcurQEDBuj111/XnDlzVKpUKf32228aOXJknofNSZefk/Pvf/9bU6dO1RNPPKF77rlHn376qdq3b6/7779f3bt3tz/E78iRI1q1apUkFXib+JXS09Pt11X8WUBAgMNoUHR0tO6++2716NFDhw8fzvOMnoiICJUuXVpxcXEaMmSI3NzcNHv2bH3//feFquOFF17QoEGDNHjwYEVGRmrnzp3697//LT8/P4d+TzzxhN58800NGTJEkZGR2rVrl4YPH66QkBCHu7xKliyp4OBgffnll2rSpInKlCkjf39/ValSJc+6S5UqpUGDBumNN95Qx44d1b59ex07dkzDhg2Tp6enhgwZUqhtuJZdu3apWbNmateunSIjI1WhQgWdOHFCS5cu1UcffaSoqChFRERIunwLfmJiotq0aaP+/fvrwQcf1Pnz55WcnKwnnnhCjRo1UseOHTV+/Hh16tRJBw4c0P33368NGzbo7bffVsuWLa96vU6usWPH6tFHH1XDhg3VvXt3ValSRWfOnNGePXu0ePFie7hq3bq1/ZlNAQEBOnjwoMaMGaPg4GBVr169SH4+QJFw8gXNgKXl3pGzZcuWPNPOnz9vKleubKpXr26eeuop4+7ubo4cOVLgstq1a2dcXV3N4cOH7W179+41L7/8sqlZs6bx8vIyHh4eJjg42Pz97383CxYsMDk5OVet71p3Sz333HN55nnjjTeMJBMUFJTv3VgpKSmmQYMGpkSJEiYgIMB069bNfPfdd0aSmT59ur1ffndLZWZmmtdff90EBQUZLy8vExkZaVJTU/PcLZWZmWleffVVU6lSJePp6Wnq1q1rFi5caDp16mSCg4MdlvnVV1+Z//u//zMeHh5Gkn05V94tlWvKlCmmdu3axt3d3fj5+Zk2bdqYHTt2OPTp1KmT8fb2zrPt+W3TlU6cOGFGjBhhGjdubCpVqmTc3d2Nt7e3eeCBB8yIESPMuXPn8vR/5ZVXTOXKlY2bm5spV66cadWqlfn555/tfY4dO2bi4uJMhQoVjKurqwkODjYDBgwwFy5ccFiWJNOzZ89869q/f7/p2rWrqVSpknFzczMBAQEmIiLCjBgxwt5n1KhRJiIiwvj7+xt3d3dTuXJlExsbaw4cOHDVbQZuN5sx1/jyGQAAgDsI19wAAABLIdwAAABLIdwAAABLcWq4WbdunVq3bq2KFSvKZrPluZUxP8nJyQoPD5enp6eqVq2qSZMm3fpCAQDAHcOp4eaPP/5QnTp19O9//7tQ/ffv36+WLVuqYcOG2r59u9544w317t1bX3zxxS2uFAAA3CmKzd1SNptNCxYs0FNPPVVgn3/+859atGiRw3fHxMXF6fvvv8/z8C4AAPDXdEc9xG/Tpk2Kjo52aGvWrJmmTp2qrKwsubm55ZknMzPT4XHuOTk5On78uMqWLZvnMeQAAKB4MsbozJkzqlixou666+onnu6ocHP48GEFBgY6tAUGBurSpUs6evRovl94l5CQoGHDht2uEgEAwC106NCha36h7B0VbqS8X/qWe1atoFGYAQMGKD4+3v7+1KlTqly5sg4dOlToR9MDAADnOn36tIKCglSyZMlr9r2jwk358uV1+PBhh7YjR47I1dU13y92ky5/cd6VX7AnXf7OHcINAAB3lsJcUnJHPeemQYMGSkpKcmhbtWqV6tWrl+/1NgAA4K/HqeHm7NmzSk1NVWpqqqTLt3qnpqYqPT1d0uVTSh07drT3j4uL08GDBxUfH6+0tDRNmzZNU6dO1auvvuqM8gEAQDHk1NNSW7duVaNGjezvc6+N6dSpk2bMmKGMjAx70JGkkJAQLVu2TH379tX48eNVsWJFjRs3Ts8888xtrx0AABRPxeY5N7fL6dOn5efnp1OnTnHNDQAAd4jr+f19R11zAwAAcC131N1Sd4Iq/Zc6uwQ42YF3Wjm7BAD4S2PkBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWAq3ggMWw+MIwOMI8FfHyA0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUp4ebCRMmKCQkRJ6engoPD9f69euv2n/27NmqU6eOSpQooQoVKqhLly46duzYbaoWAAAUd04NN4mJierTp48GDhyo7du3q2HDhmrRooXS09Pz7b9hwwZ17NhRsbGx2rFjhz777DNt2bJF3bp1u82VAwCA4sqp4Wb06NGKjY1Vt27dFBoaqjFjxigoKEgTJ07Mt/8333yjKlWqqHfv3goJCdGjjz6ql156SVu3br3NlQMAgOLKaeHm4sWL2rZtm6Kjox3ao6OjlZKSku88ERER+vXXX7Vs2TIZY/S///1Pn3/+uVq1alXgejIzM3X69GmHFwAAsC6nhZujR48qOztbgYGBDu2BgYE6fPhwvvNERERo9uzZiomJkbu7u8qXL69SpUrpww8/LHA9CQkJ8vPzs7+CgoKKdDsAAEDx4vQLim02m8N7Y0yetlw7d+5U7969NXjwYG3btk0rVqzQ/v37FRcXV+DyBwwYoFOnTtlfhw4dKtL6AQBA8eLqrBX7+/vLxcUlzyjNkSNH8ozm5EpISNAjjzyi1157TZJUu3ZteXt7q2HDhhoxYoQqVKiQZx4PDw95eHgU/QYAAIBiyWkjN+7u7goPD1dSUpJDe1JSkiIiIvKd59y5c7rrLseSXVxcJF0e8QEAAHDqaan4+HhNmTJF06ZNU1pamvr27av09HT7aaYBAwaoY8eO9v6tW7fW/PnzNXHiRO3bt08bN25U79699eCDD6pixYrO2gwAAFCMOO20lCTFxMTo2LFjGj58uDIyMhQWFqZly5YpODhYkpSRkeHwzJvOnTvrzJkz+ve//61+/fqpVKlSaty4sUaOHOmsTQAAAMWMzfzFzuecPn1afn5+OnXqlHx9fYt8+VX6Ly3yZeLOcuCdgh9NcDuwD8LZ+yBwK1zP72+n3y0FAABQlAg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUgg3AADAUpwebiZMmKCQkBB5enoqPDxc69evv2r/zMxMDRw4UMHBwfLw8NA999yjadOm3aZqAQBAcefqzJUnJiaqT58+mjBhgh555BFNnjxZLVq00M6dO1W5cuV853n22Wf1v//9T1OnTlW1atV05MgRXbp06TZXDgAAiiunhpvRo0crNjZW3bp1kySNGTNGK1eu1MSJE5WQkJCn/4oVK5ScnKx9+/apTJkykqQqVarczpIBAEAx57TTUhcvXtS2bdsUHR3t0B4dHa2UlJR851m0aJHq1aund999V5UqVVKNGjX06quv6vz58wWuJzMzU6dPn3Z4AQAA63LayM3Ro0eVnZ2twMBAh/bAwEAdPnw433n27dunDRs2yNPTUwsWLNDRo0fVo0cPHT9+vMDrbhISEjRs2LAirx8AABRPTr+g2GazObw3xuRpy5WTkyObzabZs2frwQcfVMuWLTV69GjNmDGjwNGbAQMG6NSpU/bXoUOHinwbAABA8eG0kRt/f3+5uLjkGaU5cuRIntGcXBUqVFClSpXk5+dnbwsNDZUxRr/++quqV6+eZx4PDw95eHgUbfEAAKDYctrIjbu7u8LDw5WUlOTQnpSUpIiIiHzneeSRR/Tbb7/p7Nmz9rZffvlFd911l+6+++5bWi8AALgzOPW0VHx8vKZMmaJp06YpLS1Nffv2VXp6uuLi4iRdPqXUsWNHe/8OHTqobNmy6tKli3bu3Kl169bptddeU9euXeXl5eWszQAAAMWIU28Fj4mJ0bFjxzR8+HBlZGQoLCxMy5YtU3BwsCQpIyND6enp9v4+Pj5KSkrSyy+/rHr16qls2bJ69tlnNWLECGdtAgAAKGacGm4kqUePHurRo0e+02bMmJGn7d57781zKgsAACCX0++WAgAAKEqEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCk3FW4uXryoXbt26dKlS0VVDwAAwE25oXBz7tw5xcbGqkSJErrvvvuUnp4uSerdu7feeeedIi0QAADgetxQuBkwYIC+//57rV27Vp6envb2pk2bKjExsciKAwAAuF6uNzLTwoULlZiYqIcfflg2m83eXqtWLe3du7fIigMAALheNzRy8/vvv6tcuXJ52v/44w+HsAMAAHC73VC4qV+/vpYuXWp/nxtoPv74YzVo0KBoKgMAALgBN3RaKiEhQc2bN9fOnTt16dIljR07Vjt27NCmTZuUnJxc1DUCAAAU2g2N3ERERCglJUXnzp3TPffco1WrVikwMFCbNm1SeHh4UdcIAABQaNc9cpOVlaUXX3xRgwYN0syZM29FTQAAADfsukdu3NzctGDBgltRCwAAwE27odNSbdu21cKFC4u4FAAAgJt3QxcUV6tWTW+++aZSUlIUHh4ub29vh+m9e/cukuIAAACu1w2FmylTpqhUqVLatm2btm3b5jDNZrMRbgAAgNPcULjZv39/UdcBAABQJG7qW8ElyRgjY0xR1AIAAHDTbjjczJo1S/fff7+8vLzk5eWl2rVr6z//+U9R1gYAAHDdbui01OjRozVo0CD16tVLjzzyiIwx2rhxo+Li4nT06FH17du3qOsEAAAolBsKNx9++KEmTpyojh072tvatGmj++67T0OHDiXcAAAAp7mh01IZGRmKiIjI0x4REaGMjIybLgoAAOBG3VC4qVatmubNm5enPTExUdWrV7/pogAAAG7UDZ2WGjZsmGJiYrRu3To98sgjstls2rBhg1avXp1v6AEAALhdbmjk5plnntHmzZvl7++vhQsXav78+fL399e3336rtm3bFnWNAAAAhXZDIzeSFB4erk8++aQoawEAALhpNzRys2zZMq1cuTJP+8qVK7V8+fKbLgoAAOBG3VC46d+/v7Kzs/O0G2PUv3//my4KAADgRt1QuNm9e7dq1aqVp/3ee+/Vnj17brooAACAG3VD4cbPz0/79u3L075nzx55e3vfdFEAAAA36obCzZNPPqk+ffpo79699rY9e/aoX79+evLJJ4usOAAAgOt1Q+Hmvffek7e3t+69916FhIQoJCRE9957r8qWLav333+/qGsEAAAotBu6FdzPz08pKSlKSkrS999/Ly8vL9WpU0cNGzYs6voAAACuy3WN3GzevNl+q7fNZlN0dLTKlSun999/X88884xefPFFZWZm3pJCAQAACuO6ws3QoUP1ww8/2N//+OOP+sc//qHHH39c/fv31+LFi5WQkFDkRQIAABTWdYWb1NRUNWnSxP5+7ty5evDBB/Xxxx8rPj5e48aN47ulAACAU11XuDlx4oQCAwPt75OTk9W8eXP7+/r16+vQoUNFVx0AAMB1uq5wExgYqP3790uSLl68qO+++04NGjSwTz9z5ozc3NyKtkIAAIDrcF3hpnnz5urfv7/Wr1+vAQMGqESJEg53SP3www+65557irxIAACAwrquW8FHjBihp59+WpGRkfLx8dHMmTPl7u5unz5t2jRFR0cXeZEAAACFdV3hJiAgQOvXr9epU6fk4+MjFxcXh+mfffaZfHx8irRAAACA63HDD/HLT5kyZW6qGAAAgJt1Q1+/AAAAUFwRbgAAgKUQbgAAgKUQbgAAgKUQbgAAgKUQbgAAgKUQbgAAgKUQbgAAgKUQbgAAgKUQbgAAgKUQbgAAgKU4PdxMmDBBISEh8vT0VHh4uNavX1+o+TZu3ChXV1c98MADt7ZAAABwR3FquElMTFSfPn00cOBAbd++XQ0bNlSLFi2Unp5+1flOnTqljh07qkmTJrepUgAAcKdwargZPXq0YmNj1a1bN4WGhmrMmDEKCgrSxIkTrzrfSy+9pA4dOqhBgwa3qVIAAHCncFq4uXjxorZt26bo6GiH9ujoaKWkpBQ43/Tp07V3714NGTKkUOvJzMzU6dOnHV4AAMC6nBZujh49quzsbAUGBjq0BwYG6vDhw/nOs3v3bvXv31+zZ8+Wq6trodaTkJAgPz8/+ysoKOimawcAAMWX0y8ottlsDu+NMXnaJCk7O1sdOnTQsGHDVKNGjUIvf8CAATp16pT9dejQoZuuGQAAFF+FG/64Bfz9/eXi4pJnlObIkSN5RnMk6cyZM9q6dau2b9+uXr16SZJycnJkjJGrq6tWrVqlxo0b55nPw8NDHh4et2YjAABAseO0kRt3d3eFh4crKSnJoT0pKUkRERF5+vv6+urHH39Uamqq/RUXF6eaNWsqNTVVDz300O0qHQAAFGNOG7mRpPj4eL3wwguqV6+eGjRooI8++kjp6emKi4uTdPmU0n//+1/NmjVLd911l8LCwhzmL1eunDw9PfO0AwCAvy6nhpuYmBgdO3ZMw4cPV0ZGhsLCwrRs2TIFBwdLkjIyMq75zBsAAIA/sxljjLOLuJ1Onz4tPz8/nTp1Sr6+vkW+/Cr9lxb5MnFnOfBOK6eun30Qzt4HgVvhen5/O/1uKQAAgKJEuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJbi6uwCAADWUqX/UmeXACc78E4rp66fkRsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGApTg83EyZMUEhIiDw9PRUeHq7169cX2Hf+/Pl6/PHHFRAQIF9fXzVo0EArV668jdUCAIDizqnhJjExUX369NHAgQO1fft2NWzYUC1atFB6enq+/detW6fHH39cy5Yt07Zt29SoUSO1bt1a27dvv82VAwCA4sqp4Wb06NGKjY1Vt27dFBoaqjFjxigoKEgTJ07Mt/+YMWP0+uuvq379+qpevbrefvttVa9eXYsXL77NlQMAgOLKaeHm4sWL2rZtm6Kjox3ao6OjlZKSUqhl5OTk6MyZMypTpkyBfTIzM3X69GmHFwAAsC6nhZujR48qOztbgYGBDu2BgYE6fPhwoZYxatQo/fHHH3r22WcL7JOQkCA/Pz/7Kygo6KbqBgAAxZvTLyi22WwO740xedry8+mnn2ro0KFKTExUuXLlCuw3YMAAnTp1yv46dOjQTdcMAACKL1dnrdjf318uLi55RmmOHDmSZzTnSomJiYqNjdVnn32mpk2bXrWvh4eHPDw8brpeAABwZ3DayI27u7vCw8OVlJTk0J6UlKSIiIgC5/v000/VuXNnzZkzR61atbrVZQIAgDuM00ZuJCk+Pl4vvPCC6tWrpwYNGuijjz5Senq64uLiJF0+pfTf//5Xs2bNknQ52HTs2FFjx47Vww8/bB/18fLykp+fn9O2AwAAFB9ODTcxMTE6duyYhg8froyMDIWFhWnZsmUKDg6WJGVkZDg882by5Mm6dOmSevbsqZ49e9rbO3XqpBkzZtzu8gEAQDHk1HAjST169FCPHj3ynXZlYFm7du2tLwgAANzRnH63FAAAQFEi3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEtxeriZMGGCQkJC5OnpqfDwcK1fv/6q/ZOTkxUeHi5PT09VrVpVkyZNuk2VAgCAO4FTw01iYqL69OmjgQMHavv27WrYsKFatGih9PT0fPvv379fLVu2VMOGDbV9+3a98cYb6t27t7744ovbXDkAACiunBpuRo8erdjYWHXr1k2hoaEaM2aMgoKCNHHixHz7T5o0SZUrV9aYMWMUGhqqbt26qWvXrnr//fdvc+UAAKC4clq4uXjxorZt26bo6GiH9ujoaKWkpOQ7z6ZNm/L0b9asmbZu3aqsrKxbVisAALhzuDprxUePHlV2drYCAwMd2gMDA3X48OF85zl8+HC+/S9duqSjR4+qQoUKeebJzMxUZmam/f2pU6ckSadPn77ZTchXTua5W7Jc3Dlu1b5VWOyDYB+Es92KfTB3mcaYa/Z1WrjJZbPZHN4bY/K0Xat/fu25EhISNGzYsDztQUFB11sqUCh+Y5xdAf7q2AfhbLdyHzxz5oz8/Pyu2sdp4cbf318uLi55RmmOHDmSZ3QmV/ny5fPt7+rqqrJly+Y7z4ABAxQfH29/n5OTo+PHj6ts2bJXDVG4fqdPn1ZQUJAOHTokX19fZ5eDvyD2QTgb++CtY4zRmTNnVLFixWv2dVq4cXd3V3h4uJKSktS2bVt7e1JSktq0aZPvPA0aNNDixYsd2latWqV69erJzc0t33k8PDzk4eHh0FaqVKmbKx5X5evry4caTsU+CGdjH7w1rjVik8upd0vFx8drypQpmjZtmtLS0tS3b1+lp6crLi5O0uVRl44dO9r7x8XF6eDBg4qPj1daWpqmTZumqVOn6tVXX3XWJgAAgGLGqdfcxMTE6NixYxo+fLgyMjIUFhamZcuWKTg4WJKUkZHh8MybkJAQLVu2TH379tX48eNVsWJFjRs3Ts8884yzNgEAABQzNlOYy46BQsjMzFRCQoIGDBiQ51QgcDuwD8LZ2AeLB8INAACwFKd/txQAAEBRItwAAABLIdwAAABLIdxYXJUqVTRmzJjbus7OnTvrqaeeuq3rvJoDBw7IZrMpNTVVkrR27VrZbDadPHnSqXUBuL2ioqLUp08f+/uiPj7ezmOfM47tdxLCzW3UuXNn2Ww22Ww2ubq6qnLlyurevbtOnDhR6GVc+Yv6WrZs2aIXX3zxBiu+dbKzs/XBBx+odu3a8vT0VKlSpdSiRQtt3Ljxlq87IiJCGRkZ9odBzZgxwzIPdkxJSZGLi4uaN2/u7FKKlDFGH330kR566CH5+PioVKlSqlevnsaMGaNz54rn9xjZbDYtXLjQ2WVYQkGhobj9oTJ27FjNmDGjSJdZ0PGpuB7biwvCzW3WvHlzZWRk6MCBA5oyZYoWL16sHj16FPl6Ll68KEkKCAhQiRIlinz5N8MYo3bt2mn48OHq3bu30tLSlJycrKCgIEVFRd3yXwju7u4qX768Jb9+Y9q0aXr55Ze1YcMGh2dE3QmysrIKnPbCCy+oT58+atOmjb7++mulpqZq0KBB+vLLL7Vq1aoiXWfuZwe4Xn5+frftD6XieGwvVgxum06dOpk2bdo4tMXHx5syZco4tE2bNs3ce++9xsPDw9SsWdOMHz/ePk2SwysyMtJh2W+//bapUKGCCQ4ONsYYExwcbD744AP7/CdPnjT/+Mc/TEBAgClZsqRp1KiRSU1NNcYY8/PPPxtJJi0tzaGeUaNGmeDgYJOTk2MuXbpkunbtaqpUqWI8PT1NjRo1zJgxY665nX82d+5cI8ksWrQoz7Snn37alC1b1pw9e7bAZb3yyiv27TbGmOXLl5tHHnnE+Pn5mTJlyphWrVqZPXv22Kfv37/fSDLbt283xhjz9ddfG0nmxIkT9n//+TVkyBAzbNgwExYWlqe+unXrmkGDBhW4bc509uxZU7JkSfPzzz+bmJgYM2zYMIfpudv61VdfmfDwcOPl5WUaNGhgfv75Z3uf1NRUExUVZXx8fEzJkiVN3bp1zZYtW0xOTo7x9/c3n3/+ub1vnTp1TEBAgP19SkqKcXV1NWfOnDHGXH1fM8aYIUOGmDp16pipU6eakJAQY7PZTE5OTp7tSkxMNJLMwoUL80zLyckxJ0+eNMYYk52dbYYNG2YqVapk3N3dTZ06dczy5cvtfXP3g8TERBMZGWk8PDzMtGnTCvzs/Prrr+bZZ581pUqVMmXKlDFPPvmk2b9/v8P6p06damrVqmXc3d1N+fLlTc+ePY0xlz93f96ncpeZu82zZs0ywcHBxtfX18TExJjTp087bNPIkSNNSEiI8fT0NLVr1zafffaZffrx48dNhw4djL+/v/H09DTVqlUz06ZNM8YYk5mZaXr27GnKly9vPDw8THBwsHn77bfz/NzuNAUdU/78WT569Khp166dqVSpkvHy8jJhYWFmzpw5Dv0jIyPNK6+8Yn//5+PjlccJY4w5ceKEkWS+/vpre9tPP/1kWrZsaUqWLGl8fHzMo48+aj/eXFlnZGSkefnll81rr71mSpcubQIDA82QIUMcaho1apQJCwszJUqUMHfffbfp3r27/TNU0PHpytqNMebgwYPmySefNN7e3qZkyZLm73//uzl8+LB9emH2PSth5MaJ9u3bpxUrVjh8L9bHH3+sgQMH6q233lJaWprefvttDRo0SDNnzpQkffvtt5Kkr776ShkZGZo/f7593tWrVystLU1JSUlasmRJnvUZY9SqVSsdPnxYy5Yt07Zt21S3bl01adJEx48fV82aNRUeHq7Zs2c7zDdnzhx16NBBNptNOTk5uvvuuzVv3jzt3LlTgwcP1htvvKF58+YVervnzJmjGjVqqHXr1nmm9evXT8eOHVNSUlKhl/fHH38oPj5eW7Zs0erVq3XXXXepbdu2ysnJuea8ERERGjNmjHx9fZWRkaGMjAy9+uqr6tq1q3bu3KktW7bY+/7www/avn27OnfuXOjabqfExETVrFlTNWvW1PPPP6/p06fL5PMYq4EDB2rUqFHaunWrXF1d1bVrV/u05557Tnfffbe2bNmibdu2qX///nJzc5PNZtNjjz2mtWvXSpJOnDihnTt3KisrSzt37pR0+RRBeHi4fHx8rrmv5dqzZ4/mzZunL774osBTrbNnz1bNmjXz/c45m81mP704duxYjRo1Su+//75++OEHNWvWTE8++aR2797tMM8///lP+4hhs2bNJOX97Jw7d06NGjWSj4+P1q1bpw0bNsjHx0fNmze3j+xMnDhRPXv21Isvvqgff/xRixYtUrVq1STJvt9Mnz5dGRkZDvvR3r17tXDhQi1ZskRLlixRcnKy3nnnHfv0f/3rX5o+fbomTpyoHTt2qG/fvnr++eeVnJwsSRo0aJB27typ5cuXKy0tTRMnTpS/v78kady4cVq0aJHmzZunXbt26ZNPPlGVKlXy/blazYULFxQeHq4lS5bop59+0osvvqgXXnhBmzdvLrJ1/Pe//9Vjjz0mT09PrVmzRtu2bVPXrl116dKlAueZOXOmvL29tXnzZr377rsaPny4w/Htrrvu0rhx4/TTTz9p5syZWrNmjV5//XVJBR+frmSM0VNPPaXjx48rOTlZSUlJ2rt3r2JiYhz6XWvfsxTnZqu/lk6dOhkXFxfj7e1tPD097Ul89OjR9j5BQUF5/tp48803TYMGDYwx+f91kbvswMBAk5mZ6dD+53S/evVq4+vray5cuODQ55577jGTJ082xhgzevRoU7VqVfu0Xbt2GUlmx44dBW5Xjx49zDPPPONQy9VGbu69994Cpx8/ftxIMiNHjixwWVeO3FzpyJEjRpL58ccfjTFXH7kxxpjp06cbPz+/PMtp0aKF6d69u/19nz59TFRUVIHrdbaIiAj7KFpWVpbx9/c3SUlJ9ul/HrnJtXTpUiPJnD9/3hhjTMmSJc2MGTPyXf64cePso1kLFy409erVM08//bR9ZDE6Otr885//NMYUbl8bMmSIcXNzM0eOHLnqdoWGhponn3zymttfsWJF89Zbbzm01a9f3/To0cMY8//3g/xGGq/87EydOtXUrFnTYSQpMzPTeHl5mZUrV9rXN3DgwALrkWQWLFjg0DZkyBBTokQJh7+WX3vtNfPQQw8ZYy6Pvnl6epqUlBSH+WJjY0379u2NMca0bt3adOnSJd91vvzyy6Zx48b5joDdyf587PzzK/c4mvtZvlLLli1Nv3797O9vduRmwIABJiQkxFy8eLHAOq8cuXn00Ucd+tSvX9/+OcnPvHnzTNmyZe3vCzo+/bn2VatWGRcXF5Oenm6fvmPHDiPJfPvtt8aYa+97VsPIzW3WqFEjpaamavPmzXr55ZfVrFkzvfzyy5Kk33//XYcOHVJsbKx8fHzsrxEjRmjv3r3XXPb9998vd3f3Aqdv27ZNZ8+eVdmyZR2Wv3//fvvy27Vrp4MHD+qbb76RdPmv5gceeEC1atWyL2fSpEmqV6+eAgIC5OPjo48//rjIr++42nZcae/everQoYOqVq0qX19fhYSESNJN1/SPf/xDn376qS5cuKCsrCzNnj3bYZSjONm1a5e+/fZbtWvXTpLk6uqqmJgYTZs2LU/f2rVr2/9doUIFSdKRI0ckXf4y227duqlp06Z65513HPa7qKgo7dixQ0ePHlVycrKioqIUFRWl5ORkXbp0SSkpKYqMjJRUuH1NkoKDgxUQEHDVbTPGXPP6qNOnT+u3337TI4884tD+yCOPKC0tzaGtXr16eea/8rOzbds27dmzRyVLlrTXXqZMGV24cEF79+7VkSNH9Ntvv6lJkyZXrSs/VapUUcmSJe3vK1SoYP/579y5UxcuXNDjjz/u8HObNWuW/efWvXt3zZ07Vw888IBef/11paSk2JfVuXNnpaamqmbNmurdu/dNXY9U3OQeO//8mjJlin16dna23nrrLdWuXdu+361atapIj02pqalq2LChw2j7tfz58yY5/n9L0tdff63HH39clSpVUsmSJdWxY0cdO3ZMf/zxR6HXkZaWpqCgIAUFBdnbatWqpVKlSjns/1fb96zGqV+c+Vfk7e1tH7oeN26cGjVqpGHDhunNN9+0n0b5+OOP9dBDDznM5+LiUqhlX01OTo4qVKhgP7XwZ7kXwVWoUEGNGjXSnDlz9PDDD+vTTz/VSy+9ZO83b9489e3bV6NGjVKDBg1UsmRJvffee9c19Fu9enX7qYwr5X4Qa9SoIenykK254tTKlReBtm7dWkFBQfr4449VsWJF5eTkKCws7KYvDG3durU8PDy0YMECeXh4KDMzs9h+SevUqVN16dIlVapUyd5mjJGbm5tOnDih0qVL29v/fGDODQ25+97QoUPVoUMHLV26VMuXL9eQIUM0d+5ctW3bVmFhYSpbtqySk5OVnJys4cOHKygoSG+99Za2bNmi8+fP69FHH7Uv71r7mnTtfVa6vC9cGVAKcmUIyi8Y5bfOK9tycnLyPUUrXb6Q8667bvzvwit/Meae7s1dryQtXbrU4f9Skv17ilq0aKGDBw9q6dKl+uqrr9SkSRP17NlT77//vurWrav9+/dr+fLl+uqrr/Tss8+qadOm+vzzz2+43uLiz8fOXL/++qv936NGjdIHH3ygMWPG6P7775e3t7f69OlT6ONA7v/pn483Vx5rvLy8rrvuq/1/Hzx4UC1btlRcXJzefPNNlSlTRhs2bFBsbOxVL7C/UkF/AFzZfrVarIZw42RDhgxRixYt1L17d1WsWFGVKlXSvn379Nxzz+XbP/evy+zs7OteV926dXX48GG5urpe9Tz8c889p3/+859q37699u7dax8NkKT169crIiLC4Q6vwowq/Vn79u3VoUMHLV68OM91N6NGjVLFihX1+OOPS7r8i+Snn35y6JOammr/kB47dkxpaWmaPHmyGjZsKEnasGHDddXj7u6e78/T1dVVnTp10vTp0+Xh4aF27doVy7sTLl26pFmzZmnUqFGKjo52mPbMM89o9uzZ6tWrV6GXV6NGDdWoUUN9+/ZV+/btNX36dLVt29Z+3c2XX36pn376SQ0bNlTJkiWVlZWlSZMmqW7duva/Cgu7rxVGhw4d1K5dO3355Zd5rrsxxuj06dPy8/NTxYoVtWHDBj322GP26SkpKXrwwQeve51169ZVYmKiypUrJ19f33z7VKlSRatXr1ajRo3yne7m5nbdn9NatWrJw8ND6enp9lGw/AQEBKhz587q3LmzGjZsqNdee03vv/++JMnX11cxMTGKiYnR3/72NzVv3lzHjx9XmTJlrquWO8369evVpk0bPf/885IuB8Xdu3crNDS0UPPnjiBmZGTo//7v/yQpz3VgtWvX1syZM5WVlXVdozcF2bp1qy5duqRRo0bZw9WV1y8WdHz6s1q1aik9PV2HDh2yj97s3LlTp06dKvT2Ww2npZwsKipK9913n95++21Jl/9yTkhI0NixY/XLL7/oxx9/1PTp0zV69GhJUrly5eTl5aUVK1bof//7n06dOlXodTVt2lQNGjTQU089pZUrV+rAgQNKSUnRv/71L23dutXe7+mnn9bp06fVvXt3NWrUyOEvyGrVqmnr1q1auXKlfvnlFw0aNMjhYsnCaNeunZ566il16tRJU6dO1YEDB/TDDz/opZde0pIlS/TJJ5/YDxyNGzfW1q1bNWvWLO3evVtDhgxxCDulS5dW2bJl9dFHH2nPnj1as2aN4uPjr6ueKlWq6OzZs1q9erWOHj3q8NyUbt26ac2aNVq+fHmxPSW1ZMkSnThxQrGxsQoLC3N4/e1vf9PUqVMLtZzz58+rV69eWrt2rQ4ePKiNGzdqy5YtDgfHqKgozZkzR7Vr15avr6898MyePVtRUVH2foXd1wrj2WefVUxMjNq3b6+EhARt3bpVBw8e1JIlS9S0aVN9/fXXkqTXXntNI0eOVGJionbt2qX+/fsrNTVVr7zyynWtT7oc8P39/dWmTRutX79e+/fvV3Jysl555RX7aMHQoUM1atQojRs3Trt379Z3332nDz/80L6M3PBz+PDhQj/LqmTJknr11VfVt29fzZw5U3v37tX27ds1fvx4+00FgwcP1pdffqk9e/Zox44dWrJkif3/6IMPPtDcuXP1888/65dfftFnn32m8uXLW+Y5TldTrVo1JSUlKSUlRWlpaXrppZd0+PDhQs/v5eWlhx9+WO+884527typdevW6V//+pdDn169eun06dNq166dtm7dqt27d+s///mPdu3adUM133PPPbp06ZI+/PBD7du3T//5z380adIkhz5XOz7latq0qWrXrq3nnntO3333nb799lt17NhRkZGR+Z6G/Ssg3BQD8fHx+vjjj3Xo0CF169ZNU6ZM0YwZM3T//fcrMjJSM2bMsF9H4urqqnHjxmny5MmqWLFivneQFMRms2nZsmV67LHH1LVrV9WoUUPt2rXTgQMHFBgYaO/n6+ur1q1b6/vvv88zghQXF6enn35aMTExeuihh3Ts2LHrfk6PzWbTZ599pjfeeEMffPCBatasqTp16ujzzz/X9u3bHf4SbtasmQYNGqTXX39d9evX15kzZ9SxY0f79Lvuuktz587Vtm3bFBYWpr59++q99967rnoiIiIUFxenmJgYBQQE6N1337VPq169uiIiIlSzZs08pwqLi6lTp6pp06b2u4b+7JlnnlFqaqq+++67ay7HxcVFx44dU8eOHVWjRg09++yzatGihYYNG2bv06hRI2VnZzsEmcjISGVnZzuMNBR2XysMm82mOXPmaPTo0VqwYIEiIyNVu3ZtDR06VG3atLHf8dS7d2/169dP/fr10/33368VK1Zo0aJFql69+nWtT5JKlCihdevWqXLlynr66acVGhqqrl276vz58/aRnE6dOmnMmDGaMGGC7rvvPj3xxBMOd2aNGjVKSUlJCgoKso8EFMabb76pwYMHKyEhQaGhoWrWrJkWL15sPwa4u7trwIABql27th577DG5uLho7ty5kiQfHx+NHDlS9erVU/369XXgwAEtW7bspk6j3SkGDRqkunXrqlmzZoqKilL58uWv+2nB06ZNU1ZWlurVq6dXXnlFI0aMcJhetmxZrVmzRmfPnlVkZKTCw8P18ccf3/AozgMPPKDRo0dr5MiRCgsL0+zZs5WQkODQ52rHp1y5D4wsXbq0HnvsMTVt2lRVq1ZVYmLiDdVlBTZz5QUNgBN89913atq0qWJjY687nNxKxhjde++9eumll657RAgA4BzWj/O4I9StW1erV6+Wt7f3dV/Dc6scOXJEo0eP1n//+1916dLF2eUAAAqJkRugADabTf7+/ho7dqw6dOjg7HIAAIXE3VJAAcj9AHBn4rQUAACwFMINAACwFMINAACwFMINAACwFMINgL+s3IefAbAWwg0Ap+rcubNsNpvi4uLyTOvRo4dsNps6d+5cqGWtXbtWNptNJ0+eLFT/jIwMtWjR4jqqBXAnINwAcLqgoCDNnTtX58+ft7dduHBBn376qSpXrlzk68v9pujy5cvbv20bgHUQbgA4Xd26dVW5cmXNnz/f3jZ//vw838tkjNG7776rqlWrysvLy/6dZJJ04MAB+/eSlS5d2mHEJyoqSr169VJ8fLz8/f3t3zp/5WmpX3/9Ve3atVOZMmXk7e2tevXqafPmzbd46wEUNR7iB6BY6NKli6ZPn27/stZp06apa9euWrt2rb3Pv/71L82fP18TJ05U9erVtW7dOj3//PMKCAjQo48+qi+++ELPPPOMdu3aJV9fX3l5ednnnTlzprp3766NGzfm+4DG3C9DrFSpkhYtWqTy5cvru+++U05Ozi3fdgBFi3ADoFh44YUXNGDAAB04cEA2m00bN27U3Llz7eHmjz/+0OjRo7VmzRo1aNBAklS1alVt2LBBkydPVmRkpMqUKSNJKleunEqVKuWw/GrVquX7jcq55syZo99//11btmyxL6datWpFv6EAbjnCDYBiwd/fX61atdLMmTNljFGrVq3k7+9vn75z505duHDBfkop18WLFx1OXRWkXr16V52empqq//u//7MHGwB3LsINgGKja9eu6tWrlyRp/PjxDtNyTw8tXbpUlSpVcphWmIuCvb29rzr9z6ewANzZCDcAio3mzZvb72Rq1qyZw7RatWrJw8ND6enpioyMzHd+d3d3SVJ2dvZ1r7t27dqaMmWKjh8/zugNcIfjbikAxYaLi4vS0tKUlpYmFxcXh2klS5bUq6++qr59+2rmzJnau3evtm/frvHjx2vmzJmSpODgYNlsNi1ZskS///67zp49W+h1t2/fXuXLl9dTTz2ljRs3at++ffriiy+0adOmIt1GALce4QZAseLr6ytfX998p7355psaPHiwEhISFBoaqmbNmmnx4sUKCQmRJFWqVEnDhg1T//79FRgYaD/FVRju7u5atWqVypUrp5YtW+r+++/XO++8kydkASj+bCa/eyIBAADuUIzcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAAS/l/bJ4LdVF1Qt8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_scores(scores):\n",
    "    \"\"\"\n",
    "    Plot the evaluation scores.\n",
    "    \"\"\"\n",
    "    labels = [\"Retrieval Quality\", \"Answer Correctness\", \"Hallucination\"]\n",
    "    scores = [scores[\"retrieval_quality\"], scores[\"answer_correctness\"], scores[\"hallucination\"]]\n",
    "    \n",
    "    _, ax = plt.subplots()\n",
    "    ax.bar(labels, scores)\n",
    "    ax.set_xlabel('Metric')\n",
    "    # set y range to 0-1\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('RAG Evaluation Scores')\n",
    "    plt.show()\n",
    "\n",
    "plot_scores(results[\"scores\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspection of the evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'When was MadeUpCompany founded and where is it headquartered?',\n",
       "  'retrieved_context': [Document(id='6265ada3-3cce-4aae-acbd-91cd53d50e7a', metadata={'Header 1': 'About MadeUpCompany'}, page_content='[About MadeUpCompany]: \\nMadeUpCompany is a pioneering technology firm founded in 2010, specializing in cloud computing, data analytics, and machine learning. Headquartered in San Francisco, California, we have a global presence with satellite offices in New York, London, and Tokyo. Our mission is to empower businesses and individuals with cutting-edge technology that enhances efficiency, scalability, and innovation.  \\nWith a diverse team of experts from various industriesâincluding AI research, cybersecurity, and enterprise software developmentâwe push the boundaries of whatâs possible. Our commitment to continuous improvement, security, and customer success has earned us recognition as a leader in the tech space.'),\n",
       "   Document(id='326f3f95-c0a1-436a-8dcc-712bb53b7a21', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Contact'}, page_content='[About MadeUpCompany/Contact]: \\nFor more information, visit our website at www.madeupcompany.com or contact our sales team at sales@madeupcompany.com. ð'),\n",
       "   Document(id='86ee4d9f-9940-41df-a469-9c4d5d8e8137', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Why Choose Us?'}, page_content=\"[About MadeUpCompany/Why Choose Us?]: \\n- âï¸ Over 1 million satisfied users worldwide\\n- âï¸ Trusted by Fortune 500 companies\\n- âï¸ Featured in TechCrunch, Forbes, and Wired as a top innovator\\n- âï¸ Unmatched customer support and security  \\nWhether you're a startup, an enterprise, or an individual user, MadeUpCompany provides the tools you need to thrive in the digital age.\")],\n",
       "  'generated_answer': 'MadeUpCompany was founded in 2010 and its headquarters are located in San Francisco, California.',\n",
       "  'expected_answer': 'MadeUpCompany was founded in 2010 and is headquartered in San Francisco, California.',\n",
       "  'retrieval_quality': 1,\n",
       "  'answer_correctness': 1,\n",
       "  'hallucination_score': 0,\n",
       "  'retrieval_quality_reasoning': 'Fact present in Document 1',\n",
       "  'answer_correctness_reasoning': 'Identical content',\n",
       "  'hallucination_reasoning': 'Answer is directly supported'},\n",
       " {'query': 'What security features does CloudMate offer for enterprise customers?',\n",
       "  'retrieved_context': [Document(id='0a76d96c-7548-4c9e-8cbf-c4507c1763cc', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Products and Services', 'Header 3': 'CloudMate â Secure and Scalable Cloud Storage'}, page_content='[About MadeUpCompany/Products and Services/CloudMate â Secure and Scalable Cloud Storage]: \\nCloudMate is our flagship cloud storage solution, designed for businesses of all sizes. Features include:\\n- â Seamless data migration with automated backups\\n- â Military-grade encryption and multi-factor authentication\\n- â Role-based access control for enterprise security\\n- â AI-powered file organization and search capabilities'),\n",
       "   Document(id='fa706126-3776-4525-a5dc-0546ef4e729c', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Pricing', 'Header 3': 'CloudMate Plans'}, page_content='[About MadeUpCompany/Pricing/CloudMate Plans]: \\nOur secure and scalable cloud storage service, CloudMate, is available in the following plans:\\n- Basic: $9.99/month â 100GB storage, essential security features\\n- Professional: $29.99/month â 1TB storage, enhanced security, priority support\\n- Enterprise: Custom pricing â Unlimited storage, advanced compliance tools, dedicated account manager'),\n",
       "   Document(id='d6088927-ed13-47eb-b809-9b0a4798d7cf', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Products and Services'}, page_content='[About MadeUpCompany/Products and Services]: \\nWe offer a comprehensive suite of cloud-based solutions that streamline operations, enhance decision-making, and power AI-driven insights. Our core products include CloudMate, DataWiz, and Custom AI Solutions.')],\n",
       "  'generated_answer': 'CloudMate offers military-grade encryption, multi-factor authentication, role-based access control, and AI-powered file organization as its primary security features for enterprise customers. The Enterprise plan provides additional benefits like unlimited storage, advanced compliance tools, and a dedicated account manager.',\n",
       "  'expected_answer': 'CloudMate offers military-grade encryption, multi-factor authentication, and role-based access control for enterprise security.',\n",
       "  'retrieval_quality': 1,\n",
       "  'answer_correctness': 1,\n",
       "  'hallucination_score': 1,\n",
       "  'retrieval_quality_reasoning': 'Fact matches Document 1',\n",
       "  'answer_correctness_reasoning': 'More detailed than expected',\n",
       "  'hallucination_reasoning': 'Added details not in context'},\n",
       " {'query': 'How much does the Professional plan for CloudMate cost and what storage capacity does it include?',\n",
       "  'retrieved_context': [Document(id='fa706126-3776-4525-a5dc-0546ef4e729c', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Pricing', 'Header 3': 'CloudMate Plans'}, page_content='[About MadeUpCompany/Pricing/CloudMate Plans]: \\nOur secure and scalable cloud storage service, CloudMate, is available in the following plans:\\n- Basic: $9.99/month â 100GB storage, essential security features\\n- Professional: $29.99/month â 1TB storage, enhanced security, priority support\\n- Enterprise: Custom pricing â Unlimited storage, advanced compliance tools, dedicated account manager'),\n",
       "   Document(id='0a76d96c-7548-4c9e-8cbf-c4507c1763cc', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Products and Services', 'Header 3': 'CloudMate â Secure and Scalable Cloud Storage'}, page_content='[About MadeUpCompany/Products and Services/CloudMate â Secure and Scalable Cloud Storage]: \\nCloudMate is our flagship cloud storage solution, designed for businesses of all sizes. Features include:\\n- â Seamless data migration with automated backups\\n- â Military-grade encryption and multi-factor authentication\\n- â Role-based access control for enterprise security\\n- â AI-powered file organization and search capabilities'),\n",
       "   Document(id='df78dc55-83c0-48e9-b823-ed86c15514b6', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Pricing', 'Header 3': 'DataWiz Plans'}, page_content='[About MadeUpCompany/Pricing/DataWiz Plans]: \\nOur advanced data analytics platform, DataWiz, offers the following plans:\\n- Starter: $49/month â Basic analytics, limited AI insights\\n- Growth: $99/month â Advanced machine learning models, predictive analytics\\n- Enterprise: Custom pricing â Full AI customization, dedicated data scientists\\n- Custom AI Solutions â Pricing is determined based on project scope and complexity. Contact our sales team for a personalized quote.')],\n",
       "  'generated_answer': 'The Professional plan for CloudMate costs $29.99 per month and includes 1TB of storage capacity along with enhanced security features and priority support.',\n",
       "  'expected_answer': 'The Professional plan for CloudMate costs $29.99/month and includes 1TB of storage, enhanced security, and priority support.',\n",
       "  'retrieval_quality': 1,\n",
       "  'answer_correctness': 1,\n",
       "  'hallucination_score': 0,\n",
       "  'retrieval_quality_reasoning': 'Fact found in Document 1',\n",
       "  'answer_correctness_reasoning': 'Matches expected answer',\n",
       "  'hallucination_reasoning': 'Matches retrieved context'},\n",
       " {'query': 'What analytics capabilities does DataWiz provide for business intelligence?',\n",
       "  'retrieved_context': [Document(id='f1447cc9-9182-4c7b-ac16-c7d13a451592', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Products and Services', 'Header 3': 'DataWiz â Advanced Data Analytics'}, page_content='[About MadeUpCompany/Products and Services/DataWiz â Advanced Data Analytics]: \\nDataWiz transforms raw data into actionable insights using cutting-edge machine learning models. Features include:\\n- ð Predictive analytics for demand forecasting and customer behavior modeling\\n- ð Real-time dashboards with customizable reporting\\n- ð API integrations with popular business intelligence tools\\n- ð Automated anomaly detection for fraud prevention and operational efficiency'),\n",
       "   Document(id='df78dc55-83c0-48e9-b823-ed86c15514b6', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Pricing', 'Header 3': 'DataWiz Plans'}, page_content='[About MadeUpCompany/Pricing/DataWiz Plans]: \\nOur advanced data analytics platform, DataWiz, offers the following plans:\\n- Starter: $49/month â Basic analytics, limited AI insights\\n- Growth: $99/month â Advanced machine learning models, predictive analytics\\n- Enterprise: Custom pricing â Full AI customization, dedicated data scientists\\n- Custom AI Solutions â Pricing is determined based on project scope and complexity. Contact our sales team for a personalized quote.'),\n",
       "   Document(id='360be78f-dfa8-4d9f-b2d7-6334badad83b', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Roadmap'}, page_content=\"[About MadeUpCompany/Roadmap]: \\nWe are constantly evolving and introducing new features based on customer feedback. Hereâs whatâs coming soon:  \\n- ð AI-Driven Data Insights â DataWiz will introduce automated trend forecasting powered by deep learning.\\n- ð Collaboration Tools for CloudMate â Enhanced real-time document editing and team workspaces for seamless collaboration.\\n- ð Zero-Knowledge Encryption â An optional feature for businesses requiring absolute data confidentiality.  \\nWe value our customers' input and prioritize updates that deliver the most impact.\")],\n",
       "  'generated_answer': 'DataWiz provides predictive analytics for demand forecasting and customer behavior modeling, as well as real-time dashboards with customizable reporting. It also includes API integrations with popular business intelligence tools and automated anomaly detection for fraud prevention and operational efficiency.',\n",
       "  'expected_answer': 'DataWiz provides predictive analytics for demand forecasting and customer behavior modeling, real-time dashboards with customizable reporting, API integrations with popular business intelligence tools, and automated anomaly detection.',\n",
       "  'retrieval_quality': 1,\n",
       "  'answer_correctness': 1,\n",
       "  'hallucination_score': 0,\n",
       "  'retrieval_quality_reasoning': 'Fact fully described',\n",
       "  'answer_correctness_reasoning': 'Generated answer is more detailed',\n",
       "  'hallucination_reasoning': 'Answer is fully supported'},\n",
       " {'query': 'What compliance standards does MadeUpCompany adhere to?',\n",
       "  'retrieved_context': [Document(id='9edbf441-788e-45a8-947d-26c2fc76b5ac', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Security and Compliance'}, page_content='[About MadeUpCompany/Security and Compliance]: \\nSecurity is at the heart of everything we do. MadeUpCompany adheres to the highest security and regulatory standards, including:  \\n- ð GDPR, HIPAA, and SOC 2 Compliance â Ensuring global security and data protection compliance.\\n- ð End-to-End Encryption â Protecting data in transit and at rest with AES-256 encryption.\\n- ð Zero Trust Architecture â Implementing rigorous access control and continuous authentication.\\n- ð DDoS Protection & Advanced Threat Detection â Safeguarding against cyber threats with AI-powered monitoring.  \\nOur team continuously updates security measures to stay ahead of evolving cyber risks.'),\n",
       "   Document(id='6265ada3-3cce-4aae-acbd-91cd53d50e7a', metadata={'Header 1': 'About MadeUpCompany'}, page_content='[About MadeUpCompany]: \\nMadeUpCompany is a pioneering technology firm founded in 2010, specializing in cloud computing, data analytics, and machine learning. Headquartered in San Francisco, California, we have a global presence with satellite offices in New York, London, and Tokyo. Our mission is to empower businesses and individuals with cutting-edge technology that enhances efficiency, scalability, and innovation.  \\nWith a diverse team of experts from various industriesâincluding AI research, cybersecurity, and enterprise software developmentâwe push the boundaries of whatâs possible. Our commitment to continuous improvement, security, and customer success has earned us recognition as a leader in the tech space.'),\n",
       "   Document(id='326f3f95-c0a1-436a-8dcc-712bb53b7a21', metadata={'Header 1': 'About MadeUpCompany', 'Header 2': 'Contact'}, page_content='[About MadeUpCompany/Contact]: \\nFor more information, visit our website at www.madeupcompany.com or contact our sales team at sales@madeupcompany.com. ð')],\n",
       "  'generated_answer': 'MadeUpCompany adheres to GDPR, HIPAA, and SOC 2 Compliance, ensuring global security and data protection. They also implement end-to-end encryption using AES-256, employ zero trust architecture, and offer DDoS protection alongside advanced threat detection through AI-powered monitoring.',\n",
       "  'expected_answer': 'MadeUpCompany adheres to GDPR, HIPAA, and SOC 2 compliance standards for global security and data protection compliance.',\n",
       "  'retrieval_quality': 1,\n",
       "  'answer_correctness': 1,\n",
       "  'hallucination_score': 0,\n",
       "  'retrieval_quality_reasoning': 'Fact mentioned in Document 1',\n",
       "  'answer_correctness_reasoning': 'More detailed than expected',\n",
       "  'hallucination_reasoning': 'Answer is directly supported'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"individual_results\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we optimize?\n",
    "\n",
    "| Component | Importance | Parameters and Design Choices |\n",
    "|:-----------|:------------|:-------------------------------|\n",
    "| <div style=\"width:60px\">**1. Document preprocessing**</div> | <div style=\"width:150px\">Affects quality of information encoded into vector embeddings. Clean, well-structured text leads to better embeddings and more accurate retrieval.</div> | <div style=\"width:350px\">â¢ Text cleaning (HTML tags, special characters, whitespace)<br>â¢ Language detection and filtering<br>â¢ Deduplication of similar content<br>â¢ Sentence normalization (casing, punctuation)<br>â¢ Named entity recognition<br>â¢ Content filtering (boilerplate, headers/footers)<br>â¢ Metadata extraction and preservation<br>â¢ Document dating/versioning<br>â¢ OCR quality improvement<br>â¢ Table and structured data extraction</div> |\n",
    "| **2. Document chunking strategy** | Determines how documents are split into retrievable pieces, directly impacting what information is retrieved together and context boundaries. | â¢ Chunk size (token/character count)<br>â¢ Overlap between chunks<br>â¢ Semantic vs. fixed-length chunking<br>â¢ Hierarchy-aware chunking<br>â¢ Chunk methods (sentence, paragraph, section)<br>â¢ Metadata preservation within chunks<br>â¢ Document structure preservation<br>â¢ Special handling for lists, tables, code blocks<br>â¢ Entity-based chunking<br>â¢ Recursive chunking with different granularities |\n",
    "| **3. Embedding model** | Converts text into vector representations that capture semantic meaning, determining how effectively similar content can be retrieved. | â¢ Model size and performance trade-offs<br>â¢ Domain-specific vs. general-purpose embeddings<br>â¢ Multilingual vs. monolingual models<br>â¢ Fine-tuning on domain-specific data<br>â¢ Dimensionality of embeddings<br>â¢ Asymmetric embeddings (query vs. document)<br>â¢ Hybrid embedding approaches<br>â¢ Sentence vs. paragraph vs. document-level embeddings<br>â¢ Token context window limitations<br>â¢ Cost and latency considerations |\n",
    "| **4. Vector index** | Affects retrieval speed, accuracy, and scalability of the system. | â¢ Index type (HNSW, IVF, FLAT, ANNOY)<br>â¢ Quantization parameters<br>â¢ Number of lists/clusters in IVF<br>â¢ Number of neighbors in HNSW<br>â¢ Index refresh strategy<br>â¢ Sharding and distribution approach<br>â¢ Metadata filtering capabilities<br>â¢ Disk vs. memory trade-offs<br>â¢ Indexing parameters (construction time vs. quality)<br>â¢ Multi-vector indexing |\n",
    "| **5. Retrieval parameters** | Control how candidates are fetched and ranked, balancing between precision, recall, and performance. | â¢ Top-k retrieved documents<br>â¢ Distance/similarity metrics<br>â¢ Diversity parameters (MMR, clustering)<br>â¢ Hybrid retrieval (keyword + semantic)<br>â¢ Re-ranking strategies<br>â¢ Query expansion and reformulation<br>â¢ Filtering by metadata<br>â¢ Thresholding (minimum similarity score)<br>â¢ Multi-query approaches<br>â¢ Context-aware retrieval |\n",
    "| **6. RAG prompt template** | Determines how retrieved information is incorporated and used by the LLM, affecting output quality and factuality. | â¢ Few-shot examples in prompts<br>â¢ Structured vs. unstructured context inclusion<br>â¢ Citation and source attribution mechanics<br>â¢ Query-focused context summarization<br>â¢ Multi-step reasoning frameworks<br>â¢ Context compression techniques<br>â¢ Handling of conflicting information<br>â¢ Template adaptation based on document types<br>â¢ Metadata integration into prompts<br>â¢ System instructions for grounding |\n",
    "| **7. LLM model** | Processes retrieved information and generates the final response, with capabilities directly affecting output quality. | â¢ Model size and capability<br>â¢ Specialized vs. general-purpose models<br>â¢ Fine-tuning on domain-specific data<br>â¢ Instruction tuning for RAG-specific tasks<br>â¢ Context window size<br>â¢ Temperature and sampling parameters<br>â¢ Token budget allocation<br>â¢ Response format control<br>â¢ Chain-of-thought prompting compatibility<br>â¢ Tool use capabilities |\n",
    "| **8. Evaluation metrics** | Provide quantitative and qualitative assessment of system performance, guiding improvement efforts. | â¢ Faithfulness/factuality metrics<br>â¢ Relevance of retrieved documents<br>â¢ Answer correctness (against ground truth)<br>â¢ Diversity of retrieved information<br>â¢ Response completeness metrics<br>â¢ Latency and throughput measurements<br>â¢ User satisfaction metrics<br>â¢ Task-specific metrics<br>â¢ Retrieval precision, recall, and F1 scores<br>â¢ A/B testing frameworks |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
